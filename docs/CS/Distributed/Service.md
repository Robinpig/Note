## Introduction





## 注册中心

服务发现是⼀个古老的话题， 当应用开始脱离单机运行和访问时， 服务发现就诞生了。 目前的网络架构是每个主机都有⼀个独立的 IP 地址， 那么服务发现基本上都是通过某种方式获取到服务所部署的 IP 地址。
DNS 协议是最早将⼀个网络名称翻译为网络 IP 的协议， 在最初的架构选型中，DNS+LVS+Nginx 基本可以满足所有的 RESTful 服务的发现， 此时服务的 IP 列表通常配置在 nginx或者 LVS。
后来出现了 RPC 服务， 服务的上下线更加频繁， 人们开始寻求⼀种能够支持动态上下线并且推送 IP 列表变化的注册中心产品。  

Zookeeper 是⼀款经典的服务注册中心产品（虽然它最初的定位并不在于此） ， 在很长⼀段时间里，它是国人在提起 RPC 服务注册中心时心里想到的唯⼀选择， 这很大程度上与 Dubbo 在中国的普及程度有关。 
Consul 和 Eureka 都出现于 2014 年， Consul 在设计上把很多分布式服务治理上要用到的功能都包含在内， 可以支持服务注册、 健康检查、 配置管理、 Service Mesh 等。 
Eureka则借着微服务概念的流行， 与 SpringCloud 生态的深度结合， 也获取了大量的用户  
Nacos则携带着阿里巴巴大规模服务生产经验， 试图在服务注册和配置管理这个市场上， 提供给用户⼀个新的选择。  

数据模型

注册中心的核心数据是服务的名字和它对应的网络地址， 当服务注册了多个实例时， 我们需要对不健康的实例进行过滤或者针对实例的⼀些特征进行流量的分配， 那么就需要在实例上存储⼀些例如健康状态、 权重等属性。
随着服务规模的扩大， 渐渐的又需要在整个服务级别设定⼀些权限规则、以及对所有实例都生效的⼀些开关， 于是在服务级别又会设立⼀些属性。 
再往后， 我们又发现单个服务的实例又会有划分为多个子集的需求， 例如⼀个服务是多机房部署的， 那么可能需要对每个机房的实例做不同的配置， 这样又需要在服务和实例之间再设定⼀个数据级别。  



Zookeeper 没有针对服务发现设计数据模型， 它的数据是以⼀种更加抽象的树形 K-V 组织的， 因此理论上可以存储任何语义的数据。 
而 Eureka 或者 Consul 都是做到了实例级别的数据扩展， 这可以满足大部分的场景， 不过无法满足大规模和多环境的服务数据存储。  
Nacos 在经过内部多年生产经验后提炼出的数据模型， 则是⼀种服务-集群-实例的三层模型。   
Nacos 的数据模型虽然相对复杂， 但是它并不强制你使用它里面的所有数据， 在大多数场景下， 你可以选择忽略这些数据属性， 此时可以降维成和 Eureka 和 Consul ⼀样的数据模型。  



另外⼀个需要考虑的是数据的隔离模型， 作为⼀个共享服务型的组件， 需要能够在多个用户或者业
务方使用的情况下， 保证数据的隔离和安全， 这在稍微大⼀点的业务场景中非常常见。 另⼀方面服
务注册中心往往会支持云上部署， 此时就要求服务注册中心的数据模型能够适配云上的通用模型。
Zookeeper、 Consul 和 Eureka 在开源层面都没有很明确的针对服务隔离的模型， Nacos 则在⼀开始就考虑到如何让用户能够以多种维度进行数据隔离， 同时能够平滑的迁移到阿里云上对应的商业化产品。  



临时实例和持久化实例。 在定义上区分临时实例和持久化
实例的关键是健康检查的方式。 临时实例使用客户端上报模式， 而持久化实例使用服务端反向探测模式。 
临时实例需要能够自动摘除不健康实例， 而且无需持久化存储实例， 那么这种实例就适用于类 Gossip 的协议。
右边的持久化实例使用服务端探测的健康检查方式， 因为客户端不会上报心跳，那么自然就不能去自动摘除下线的实例  

在大中型的公司里， 这两种类型的服务往往都有。 ⼀些基础的组件例如数据库、 缓存等， 这些往往不能上报心跳， 这种类型的服务在注册时， 就需要作为持久化实例注册。 
而上层的业务服务， 例如微服务或者 Dubbo 服务， 服务的 Provider 端支持添加汇报心跳的逻辑， 此时就可以使用动态服务的注册方式。  



临时实例和持久实例的区分应该在service level, 一个service只能是其中之一

> Nacos1.x将持久性作为服务元数据 存在一个服务既有临时 也有持久的情况, 在2.x之后简化

数据一致性







健康检查
Zookeeper 和 Eureka 都实现了⼀种 TTL 的机制， 就是如果客户端在⼀定时间内没有向注册中心发
送心跳， 则会将这个客户端摘除。 Eureka 做的更好的⼀点在于它允许在注册服务的时候， 自定义检
查自身状态的健康检查方法。 这在服务实例能够保持心跳上报的场景下， 是⼀种比较好的体验， 在
Dubbo 和 SpringCloud 这两大体系内， 也被培养成用户心智上的默认行为。 Nacos 也支持这种
TTL 机制， 不过这与 ConfigServer 在阿里巴巴内部的机制又有⼀些区别。 Nacos 目前支持临时实
例使用心跳上报方式维持活性， 发送心跳的周期默认是 5 秒， Nacos 服务端会在 15 秒没收到心
跳后将实例设置为不健康， 在 30 秒没收到心跳时将这个临时实例摘除。
不过正如前文所说， 有⼀些服务无法上报心跳， 但是可以提供⼀个检测接口， 由外部去探测。 这样
的服务也是广泛存在的， 而且以我们的经验， 这些服务对服务发现和负载均衡的需求同样强烈。 服
务端健康检查最常见的方式是 TCP 端口探测和 HTTP 接口返回码探测， 这两种探测方式因为其协
议的通用性可以支持绝大多数的健康检查场景。 在其他⼀些特殊的场景中， 可能还需要执行特殊的
接口才能判断服务是否可用。 例如部署了数据库的主备， 数据库的主备可能会在某些情况下切换，  

需要通过服务名对外提供访问， 保证当前访问的库是主库。 此时的健康检查接口， 可能就是⼀个检
查数据库是否是主库的 MYSQL 命令了。  



客户端健康检查和服务端健康检查有⼀些不同的关注点。 客户端健康检查主要关注客户端上报心跳
的方式、 服务端摘除不健康客户端的机制。 而服务端健康检查， 则关注探测客户端的方式、 灵敏度
及设置客户端健康状态的机制。 从实现复杂性来说， 服务端探测肯定是要更加复杂的， 因为需要服
务端根据注册服务配置的健康检查方式， 去执行相应的接口， 判断相应的返回结果， 并做好重试机
制和线程池的管理。 这与客户端探测， 只需要等待心跳， 然后刷新 TTL 是不⼀样的。 同时服务端健
康检查无法摘除不健康实例， 这意味着只要注册过的服务实例， 如果不调用接口主动注销， 这些服
务实例都需要去维持健康检查的探测任务， 而客户端则可以随时摘除不健康实例， 减轻服务端的压
力  



性能与容量
虽然大部分用户用到的性能不高， 但是他们仍然希望选用的产品的性能越高越好。 影响读写性能的
因素很多： ⼀致性协议、 机器的配置、 集群的规模、 存量数据的规模、 数据结构及读写逻辑的设计
等等。 在服务发现的场景中， 我们认为读写性能都是非常关键的， 但是并非性能越高就越好， 因为
追求性能往往需要其他方面做出牺牲。 Zookeeper 在写性能上似乎能达到上万的 TPS， 这得益于
Zookeeper 精巧的设计， 不过这显然是因为有⼀系列的前提存在。 首先 Zookeeper 的写逻辑就是
进行 K-V 的写入， 内部没有聚合； 其次 Zookeeper 舍弃了服务发现的基本功能如健康检查、 友好
的查询接口， 它在支持这些功能的时候， 显然需要增加⼀些逻辑， 甚至弃用现有的数据结构； 最后，
Paxos 协议本身就限制了 Zookeeper 集群的规模， 3、 5 个节点是不能应对大规模的服务订阅和查
询的。  



Zookeeper 的容量， 从存储节点数来说， 可以达到百万级别。 不过如上面所说， 这并不代表容量的
全部， 当大量的实例上下线时， Zookeeper 的表现并不稳定， 同时在推送机制上的缺陷， 会引起客
户端的资源占用上升， 从而性能急剧下降。
Eureka 在服务实例规模在 5000 左右的时候， 就已经出现服务不可用的问题， 甚至在压测的过程中，
如果并发的线程数过高， 就会造成 Eureka crash。 不过如果服务规模在 1000 上下， 几乎目前所有
的注册中心都可以满足。  



  易用性
易用性也是用户比较关注的⼀块内容。 产品虽然可以在功能特性或者性能上做到非常先进， 但是如
果用户的使用成本极高， 也会让用户望而却步。 易用性包括多方面的工作， 例如 API 和客户端的接
入是否简单， 文档是否齐全易懂， 控制台界面是否完善等。 对于开源产品来说， 还有⼀块是社区是
否活跃。 在比较 Nacos、 Eureka 和 Zookeeper 在易用性上的表现时， 我们诚邀社区的用户进行全
方位的反馈， 因为毕竟在阿里巴巴集团内部， 我们对 Eureka、 Zookeeper 的使用场景是有限的。 从
我们使用的经验和调研来看， Zookeeper 的易用性是比较差的， Zookeeper 的客户端使用比较复杂，
没有针对服务发现的模型设计以及相应的 API 封装， 需要依赖方自己处理。 对多语言的支持也不太
好， 同时没有比较好用的控制台进行运维管理。  



集群扩展性
集群扩展性和集群容量以及读写性能关系紧密。 当使用⼀个比较小的集群规模就可以支撑远高于现
有数量的服务注册及访问时， 集群的扩展能力暂时就不会那么重要。 从协议的层面上来说， Zookee
per 使用的 ZAB 协议， 由于是单点写， 在集群扩展性上不具备优势。 Eureka 在协议上来说理论上
可以扩展到很大规模， 因为都是点对点的数据同步， 但是从我们对 Eureka 的运维经验来看，
Eureka 集群在扩容之后， 性能上有很大问题。
集群扩展性的另⼀个方面是多地域部署和容灾的支持。 当讲究集群的高可用和稳定性以及网络上的
跨地域延迟要求能够在每个地域都部署集群的时候， 我们现有的方案有多机房容灾、 异地多活、 多
数据中心等。  



多数据中心其实也算是异地多活的⼀部分。 从单个产品的维度上， Zookeeper 和 Eureka 没有给出官方的多数据中心方案。 
Nacos 基于阿里巴巴内部的使用经验， 提供的解决方案是采用 Nacos-Sync 组件来做数据中心之间的数据同步， 这意味着每个数据中心的 Nacos 集群都会有多个数据中心的全量数据。 
Nacos-Sync 是 Nacos 生态组件里的重要⼀环， 不仅会承担 Nacos 集群与 Nacos 集群之间的数据同步， 也会承担 Nacos 集群与 Eureka、 Zookeeper、 Kubernetes 及 Consul 之间的数据同步  

### 健康检查
注册中心不应该仅仅提供服务注册和发现功能，还应该保证对服务可用性进行监测，对不健康的服务和过期的进行标识或剔除，维护实例的生命周期，以保证客户端尽可能的查询到可用的服务列表
我们需要知道 个服务是否还健康。那么第 种方式是客户端主动上报，告诉服务端自己健康状态，如果在 段时间没有上报，那么我们就认为服务已经不健康。第二种，则是服务端主动向客户端进行探测，检查客户端是否还被能探测到。
在当前主流的注册中心，对于健康检查机制主要都采用了 TTL（Time To Live）机制，即客户端在定时间没有向注册中心发送心跳，那么注册中心会认为此服务不健康，进而触发后续的剔除逻辑。对于主动探测的方式那么根据不同的场景，需要采用的方式可能会有不同



在 Nacos 中，用户可以通过两种方式进行临时实例的注册，通过 Nacos 的 OpenAP 进行服务注册或通过 Nacos 提供的 SDK 进行服务注册。

对于永久实例的的监看检查，Nacos 采用的是注册中心探测机制，注册中心会在永久服务初始化时根据客户端选择的协议类型注册探活的定时任务。Nacos 现在内置提供了三种探测的协议，即Http、TCP 以及 MySQL 。 般而言 Http 和 TCP 已经可以涵盖绝大多数的健康检查场景。MySQL 主要用于特殊的业务场景，例如数据库的主备需要通过服务名对外提供访问，需要确定当前访问数据库是否为主库时，那么我们此时的健康检查接口，是 个检查数据库是否为主库的 MySQL命令。


## 负载均衡

负载均衡到底是在服务提供者实现还是在服务消费者实现， 我们看到目前的负载均衡有基于权
重、 服务提供者负载、 响应时间、 标签等策略。 其中 Ribbon 设计的客户端负载均衡机制， 主要是
选择合适现有的 IRule、 ServerListFilter 等接口实现， 或者自己继承这些接口， 实现自己的过滤逻
辑。 这里 Ribbon 采用的是两步负载均衡， 第⼀步是先过滤掉不会采用的服务提供者实例， 第二步
是在过滤后的服务提供者实例里， 实施负载均衡策略。 Ribbon 内置的几种负载均衡策略功能还是比
较强大的， 同时又因为允许用户去扩展， 这可以说是⼀种比较好的设计。  

基于标签的负载均衡策略可以做到非常灵活， Kubernetes 和 Fabio 都已经将标签运用到了对资源
的过滤中， 使用标签几乎可以实现任意比例和权重的服务流量调配。 但是标签本身需要单独的存储
以及读写功能， 不管是放在注册中心本身或者对接第三方的 CMDB  

Eureka 的
负载均衡是由 ribbon 来完成的， 而 Consul 则是由 Fabio 做负载均衡。  

## Links

