# Cache





## 前言

适用互联网高并发，高性能场景，解决mysql磁盘慢问题



虽然目前CPU缓存数据 但使用局部变量代替全局变量能拥有较小的提升(特别是在多线程环境下,使用volatile可以比较每次从内存中取和缓存中取的差别)





## 物理缓存

CPU 

L1缓存分为指令缓存和数据缓存

- L1和L2缓存在每一个CPU核中，L3则是所有CPU核心共享的内存。
- L1、L2、L3的越离CPU近就越小，速度也越快，越离CPU远，速度也越慢。
- 再往后面就是内存，内存的后面就是硬盘



L0和L4

cache line

**Tag :** 每条Cache line 前都会有一个独立分配的24bits=3Bytes来存的tag，也就是内存地址的前24bits.

**Index** : 内存地址的后面的6bits=3/4Bytes存的是这一路（way）Cache line的索引，通过6bits我们可以索引2^6=64条Cache line。

**Offset** : 在索引后面的6bits存的事Cache line的偏移量。

寻址流程

1. 用索引定位到相应的缓存块。
2. 用标签尝试匹配该缓存块的对应标签值。其结果为命中或未命中。
3. 如命中，用块内偏移定位此块内的目标字。然后直接改写这个字。
4. 如未命中，依系统设计不同可有两种处理策略，分别称为按写分配（Write allocate）和不按写分配（No-write allocate）。如果是按写分配，则先如处理读未命中一样，**将未命中数据读入缓\**\**存**，然后再将数据**写到被读入的字单元**。如果是不按写分配，则直接将数据**写回内存**。





## 在项目中缓存是如何使用的？

- 客户端缓存
- 网络缓存
- 服务应用缓存
- 数据库缓存

- 走springboot @CacheConfig

查询：先查缓存，有返回，没有查数据库，set到缓存返回。 修改：先删db，后删redis

- 单独业务使用

db，缓存一致性问题，缓存竞争后面讨论解决方案



## 缓存特征

### 命中率

### 最大容量

### 过期策略

1. FIFO
2. LRU
3. LFU
4. Others





缓存介质

1. 内存
2. 磁盘
3. 数据库

## 缓存分类

### Local Cache



#### Simple Map



#### Ehcache



Guava Cache



Spring Cache

### Distribution Cache

Memcached

Redis



## 缓存问题

### 缓存雪崩

是指缓存中数据大批量到过期时间，而查询数据量巨大，引起数据库压力过大甚至宕机

解决方案：

> - 冷热数据区分，采用不同的实效时间，缓存数据的过期时间设置随机，防止同一时间大量数据过期现象发生。
> - 如果缓存数据库是分布式部署，将热点数据均匀分布。
> - 设置热点数据永远不过期。

### 缓存穿透

是指缓存和数据库中都没有的数据，而用户不断发起请求，缓存没有起到压力缓冲的作用

> 解决方案：
>
> - 接口层增加校验，如用户鉴权校验，id做基础校验，id<=0的直接拦截；
> - 从缓存取不到的数据，在数据库中也没有取到，这时也可以将key-value对写为key-null，缓存有效时间可以设置短点，如30秒（设置太长会导致正常情况也没法使用）。这样可以- 防止攻击用户反复用同一个id暴力攻击
> - 针对key做布隆过滤器
> - 采用灰度发布的方式，先接入少量请求，再逐步增加系统的请求数量，直到全部请求都切换完成
> - 提前缓存预热或定时预热

### 缓存击穿

是指缓存中没有但数据库中有的数据（一般是缓存时间到期），这时由于并发用户特别多，同时读缓存没读到数据，又同时去数据库去取数据（sql又慢），引起数据库压力瞬间增大，造成过大压力。缓存失效时瞬时的并发打到数据库

> 解决方案：
>
> - 设置热点数据永远不过期
> - 针对key做加互斥锁  解决-第一次缓存大并发问题 单jvm级别加双重锁double-check， （使用分布式锁会限制并发能力，所以使用单jvm级别限制，特殊场景支付除外）

### 全量缓存

在处理超大规模并发的场景时，由于并发请求的数量非常大，即使少量的缓存穿透，也有可能打死数据库引发雪崩效应。

> 解决方案：
>
> - 对于这种情况，我们可以缓存全量数据来彻底避免缓存穿透问题
>
> canal订阅binlog异步更新缓存

### 缓存并发竞争

多客户端同时并发写一个key，可能本来应该先到的数据后到了，导致数据版本错了；或者是多客户端同时获取一个 key，修改值之后再写回去，只要顺序错了，数据就错了

- 如果对这个key操作，不要求顺序

这种情况下，准备一个分布式锁，大家去抢锁，抢到锁就做set操作即可，比较简单。

- 如果对这个key操作，要求顺序

假设有一个key1,系统A需要将key1设置为valueA,系统B需要将key1设置为valueB,系统C需要将key1设置为valueC. 期望按照key1的value值按照 valueA-->valueB-->valueC的顺序变化。这种时候我们在数据写入数据库的时候，需要保存一个时间戳。假设时间戳如下

```
系统A key 1 {valueA  3:00}
系统B key 1 {valueB  3:05}
系统C key 1 {valueC  3:10}
复制代码
```

那么，假设这会系统B先抢到锁，将key1设置为{valueB 3:05}。接下来系统A抢到锁，发现自己的valueA的时间戳早于缓存中的时间戳，那就不做set操作了。以此类推。

- 利用队列，将set方法变成串行访问也可以



## 缓存一致性

### Cache Aside Pattern 旁路缓存

- Read：先读缓存，如果没有命中，读数据库，再set回缓存
- 写请求：update DB, then delete Cache

### 为什么建议淘汰缓存，不修改缓存

在1和2两个并发写发生时，由于无法保证时序，此时不管先操作缓存还是先操作数据库，都可能出现：

- 请求1先操作数据库，请求2后操作数据库
- 请求2先set了缓存，请求1后set了缓存

导致，数据库与缓存之间的数据不一致。

### Cache Aside Pattern问题

Cache Aside 在高并发场景下也会出现数据不一致。 读操作A，没有命中缓存，就会到数据库中取数据v1。 此时来了一个写操作B，将v2写入数据库，让缓存失效； 读操作A在把v1放入缓存，这样就会造成脏数据。因为缓存中是v1，数据库中是v2

> 解决方案：
>
> - b线程：读缓存->未命中->上写锁>从db读数据到缓存->释放锁；a线程：上写锁->写db->删除缓存/改缓存->释放锁；
> - 看业务方能接受多长时间的脏数据，然后缓存就设置多久的过期时间。
> - 或者数据库更新成功后，用MQ去通知刷新缓存

> - canal订阅binlog，终极方案-还可以解决主从库同步问题



> - 降级或补偿方案或兜底方案



`Read Through`，其实就是让你对读操作感知不到缓存层的存在。通常情况下，你会手动实现缓存的载入，但`Read Through`可能就有代理层给你捎带着做了。

再比如，`Write Through`，你不用再考虑数据库和缓存是不是同步了，代理层都给你做了，你只管往里塞数据就行。

`Read Through`和`Write Through`是不冲突的，它们可以同时存在，这样业务层的代码里就没有同步这个概念了。爽歪歪。

至于`Write Behind Caching`，意思就是先落地到缓存，然后有异步线程缓慢的将缓存中的数据落地到DB中。要用这个东西，你得评估一下你的数据是否可以丢失，以及你的缓存容量是否能够经得起业务高峰的考验。现在的操作系统、DB、甚至消息队列如Kafaka等，都会在一定程度上践行这个模式。





## Reference

1. [缓存那些事 - 美团技术团队](https://tech.meituan.com/2017/03/17/cache-about.html)