## Introduction

“Feed”在现代信息媒体世界中可以理解为“投喂”。最早web时代，用户主动从新闻网站或博客订阅，订阅器帮用户及时更新信息，然后按照时间顺序展示
现代Feed流的核心是个性化推荐，内容和用户为两大主体，通过用户与内容的匹配实现“信息找人”的展示方式

Feed流的基础核心是内容，
其次会有内容产生的时间、地点、发布者等等，社交领域中常见的还有点赞、转发、评论等更小的元信息，根据场景和业务不同，其表现方式也不尽相同。
在移动互联网中最常见3种形式：文字流、图片流、视频流

2006年 Facebook重新定义了feed，叫News Feed，我们如今广泛使用的模式和这个类似。它有几个特点：订阅源不再是某个内容，而是生产内容的人/团体。
订阅中通常夹杂非订阅内容，比如热门推荐，广告。内容也不再严格按照timeline，广泛使用智能feed排序。
新的feed流刻意不再需要主动搜索，而是主动呈现琳琅满目的内容。
它对我们了如指掌，给我们想了解的，让我们不停的刷新沉溺于此。





Feed 流产品一般有两种形态，一种是基于算法推荐，另一种是基于关注关系并按时间排列。「关注页」这种按时间排序的 Feed 流也被为 Timeline





Feed流的主要模式
- 推（Push）
- 拉（Pull）
- 推拉结合（Hybrid）


虽然推拉结合的方式看似更加合理，但是由此带来的业务复杂度就比较高了，因为用户的粉丝数是不断变化的，所以对于哪些用户使用推模式，哪些用户使用拉模式，维护起来成本就很高了

采用拉模式的话，需要拉取所有关注人的发件箱，在关注人只有几十几百个的时候，获取效率还是非常高的
但是当关注人上千以后，耗时就会增加很多，实际验证获取超过 4000 个用户的发件箱，耗时要几百 ms，并且长尾请求（也就是单次请求耗时超过 1s）的概率也会大大增加。
为了解决关注人数上千的用户拉取 Feed 效率低的问题，我们采用了分而治之的思想，在拉取之前把用户的关注人分为几组，并行拉取，这样的话就把一次性的聚合计算操作给分解成多次聚合计算操作，最后再把多次聚合计算操作的结果汇总在一起，
类似于 MapReduce 的思路。经过我们的实际验证，通过这种方法可以有效地降级关注人数上千用户拉取 Feed 的耗时，长尾请求的数量也大大减少了




智能排序基于趋势trending、热门hot、用户生产UGC 、编辑推荐PGC、相似Similarity等等因素综合考虑，随着技术的进步智能算法将会更加懂得用户的喜好


微博诞生于 2009 年，并在 2010 年进行了平台化改造，确定了现在这一套存储和业务架构。当时的存储成熟度决定了 MySQL + Memcached 的组合是最优解，并没有使用 Redis、HBase 等后来更为先进的数据库；
同时由于微博 Feed 的业务特点，选择了拉模式的业务架构，并针对关注人数上千的场景进行了拉取方式的优化，也被实践证明是行之有效的手段



## 分页

Feed 流是一个动态的列表，列表内容会随着时间不断变化。传统的 limit + offset 分页器会有一些问题

解决这个问题的方法是根据上一页最后一条 Feed 的 ID 来拉取下一页 使用 Feed ID 来分页需要先根据 ID 查找 Feed，然后再根据 Feed 的发布时间读取下一页，流程比较麻烦。若作为分页游标的 Feed 被删除了，就更麻烦了

如果使用时间戳作为分页ID 也会存在时间戳相同的情况 可以使用时间戳 + Feed ID


## 微博


微博的存储除了大量使用 MySQL 和 Memcached 以外，还有一种存储也被广泛使用，那就是 Redis
并且基于微博自身的业务特点，我们对原生的 Redis 进行了改造，因此诞生了两类主要的 Redis 存储组件：CounterService 和 Phantom

CounterService 的主要应用场景就是计数器，比如微博的转发、评论、赞的计数
早期微博曾采用了 Redis 来存储微博的转发、评论、赞计数，
但随着微博的数据量越来越大，发现 Redis 内存的有效负荷还是比较低的，它一条 KV 大概需要至少 65 个字节，
但实际上一条微博的计数 Key 需要 8 个字节，Value 大概 4 个字节，实际上有效的只有 12 个字节，
其余四十多个字节都是被浪费的。这还只是单个 KV，如果一条微博有多个计数的情况下，
它的浪费就更多了，比如转评赞三个计数，一个 Key 是 long 结构，占用 8 个字节，每个计数是 int 结构，占用 4 个字节，三个计数大概需要 20 个字节就够了；而使用 Redis 的话，需要将近 200 个字节
正因为如此，我们研发了 CounterService，相比 Redis 来说 它的内存使用量减少到原来的 1/15～1/5。而且还进行了冷热数据分离，
热数据放到内存里，冷数据放到磁盘上，并使用 LRU，如果冷数据重新变热，就重新放到内存中

CounterService 的存储结构上面是内存下面是 SSD，预先把内存分成 N 个 Table，每个 Table 根据微博 ID 的指针序列，划出一定范围
任何一个微博 ID 过来先找到它所在的 Table，如果有的话，直接对它进行增减；如果没有，就新增加一个 Key
有新的微博 ID 过来，发现内存不够的时候，就会把最小的 Table dump 到 SSD 里面去，留着新的位置放在最上面供新的微博 ID 来使用
如果某一条微博特别热，转发、评论或者赞计数超过了 4 个字节，计数变得很大该怎么处理呢？
对于超过限制的，我们把它放在 Aux Dict 进行存放，对于落在 SSD 里面的 Table，我们有专门的 Index 进行访问，通过 RDB 方式进行复制



微博还有一种场景是“存在性判断”，比如某一条微博某个用户是否赞过、某一条微博某个用户是否看过之类的这种场景有个很大的特点，它检查是否存在，因此每条记录非常小， 比如 Value 用 1 个位存储就够了，但总数据量又非常巨大
比如每天新发布的微博数量在 1 亿条左右，是否被用户读过的总数据量可能有上千亿，怎么存储是个非常大的挑战
而且还有一个特点是，大多数微博是否被用户读过的存在性都是 0，如果存储 0 的话，每天就得存上千亿的记录；
如果不存的话，就会有大量的请求最终会穿透 Cache 层到 DB 层，任何 DB 都没有办法抗住那么大的流量

假设每天要存储上千亿条记录，用原生的 Redis 存储显然是不可行的，因为原生的 Redis，单个 KV 就占了 65 个字节，这样每天存储上千亿条记录，需要增加将近 6TB 存储，显然是不可接受的
而用上面提到的微博自研的 CounterService 来存储的话，一个 Key 占 8 个字节，Value 用 1 个位存储就够了，一个 KV 就占大约 8 个字节，这样每天存储上千亿条记录，需要增加将近 800GB 存储
虽然相比于原生的 Redis 存储方案，已经节省了很多，但存储成本依然很高，每天将近 1TB

Phantom 跟 CounterService 一样，采取了分 Table 的存储方案，不同的是 CounterService 中每个 Table 存储的是 KV，
而 Phantom 的每个 Table 是一个完整的 BloomFilter，每个 BloomFilter 存储的某个 ID 范围段的 Key，所有 Table 形成一个列表并按照 Key 范围有序递增
当所有 Table 都存满的时候，就把最小的 Table 数据清除，存储最新的 Key，这样的话最小的 Table 就滚动成为最大的 Table 了

Phantom 正是通过把内存分成 N 个 Table，每一个 Table 内使用 BloomFilter 判断是否存在，最终每天使用的内存只有 120GB。而存在性判断的业务场景最高需要满足一周的需求，所以最多使用的内存也就是 840GB

## Links



## References

1. [feed流设计：那些谋杀你时间APP | 人人都是产品经理](https://www.woshipm.com/pd/773523.html)
1. [Feed 流系统的架构设计方案 | 炫仔的Blog](https://kixuan.github.io/posts/feed/)
