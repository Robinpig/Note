## Introduction

即时消息（Instant Messaging，IM），又称实时消息，是一个可以支持在线交流的常见功能场景
IM并不是一门仅限于聊天、社交的技术 包括聊天、直播、在线客服、物联网等这些业务领域在内，所有需要“实时互动”“高实时性”的场景，都需要IM技术

IM 应用场景和业务功能
- QQ、微信等聊天类场景： 即时通讯
- 豆瓣、知乎、小红书等社区类场景：点对点、群组聊天
- yy、抖音等直播类场景：直播互动、实时弹幕
- 小米、华为、海康智能家居IoT场景：实时监控、远程控制
- 盛大、畅游等游戏类场景： 多人互动
- 滴滴、Uber等交通类场景： 位置共享
- 新东方、网易云课堂等教学类场景： 在线白板


IM并不是一项全新的技术，而是众多前后端技术的综合体，并因自身使用场景的不同，在一些技术点上有更多侧重


我第一次接触IM系统并不是和“人”相关的场景，当时就职的公司做的是一个类似“物联网”的油罐车实时追踪控制系统，一是通过GPS实时跟踪油罐车的位置，判断是否按常规路线行进，一是在油罐车到达目的地之后，通过系统远程控制开锁。
所以，这里的交互实际是“车”和“系统”的互动，当然这个系统实现上并没有多大的技术挑战，除了GPS的漂移纠偏带来了一些小小的困扰，最大的挑战莫过于调试的时候需要现场跟车调试多地奔波。
再后来，由于工作的变动，我逐渐接触到IM系统中一些高并发的业务场景，千万级实时在线用户，百亿级消息下推量，突发热点的直线峰值等。一步一步地踩坑和重构，除了感受到压力之外，对IM系统也有了更深层次的理解。
记得几年前，由于消息图片服务稳定性不好，图片消息的渲染比较慢，用户体验不好。
而且，由于图片流和文本流在同一个TCP通道，TCP的阻塞有时还会影响文本消息的收发。
所以后来我们对通道进行了一次拆分，把图片、文件等二进制流拆到一个独立通道，核心通道只推缩略图流，大幅减轻了通道压力，提升了核心链路的稳定性。
同时，独立的通道也缩短了客户端到文件流的链路，这样也提升了图片的访问性能。
但后来视频功能上线后，我们发现视频的PSR1（1秒内播放成功率）比较低，原因是视频文件一般比较大，为避免通道阻塞，不会通过消息收发的核心通道来推送。
所以之前的策略是：通过消息通道只下推视频的ID，用户真正点击播放时才从服务端下载，这种模式虽然解决了通道阻塞的问题，但“播放时再下载”容易出现卡顿的情况。
因此，针对视频类消息，我们增加了一个notify-pull模式，当客户端收到一条视频类消息的通知时，会再向服务器发起一个短连接的拉取请求，缓冲前N秒的数据。
这样等用户点击播放时，基本就能秒播了，较大地提升了视频消息播放的PSR1（1秒内播放成功率）。

我们要打造一套“实时、安全、稳定”的IM系统，我们需要深入思考很多个地方，尤其是作为整个实时互动业务的基础设施，扩展性、可用性、安全性等方面都需要有较高的保障。
比如下面几种情况。
- 某个明星忽然开直播了，在线用户数和消息数瞬间暴涨，该如何应对？
- 弱网情况下，怎么解决消息收发失败的问题，提升消息到达率？
- 如何避免敏感聊天内容由于网络劫持而泄露？



很多应用场景中都会将IM 看作是一个可以融入到各种业务系统中提供实时交互的模块


核心场景 A客户端发送消息到服务器 服务器发送下行消息到客户端B

基本要求: 可达有序 不重不漏（QoS2 + 时序性）
-消息要及时可达
-消息不重复遗漏
-消息时序 接收端消息保证是按发送时间排序



即时消息系统中，消息作为互动的载体，是必不可少的要素之一。
一般来说，大部分即时消息系统为了便于查看历史消息或者用于暂存离线消息，都需要对消息进行服务端存储
以点对点消息为例 收发双方的历史消息都是相互独立的 首先是一张消息内容表，然后是消息收发双发各自会在索引表中存储一条记录

一般 IM 系统还需要一个联系人列表 用于互动双方快速查找到需要聊天的对象 联系人列表还会携带双方最近一条记录用于展示 与历史消息存储的索引表不同 消息联系人表查询是用户全局维度
索引表通常是插入操作 消息联系人表是更新操作


消息接收通道

IM服务端的网关需要和消息接收者设备之间维护一条TCP长链接（或者是websocket长链接）通过长链接投递消息 当接收方不在线（网络不通、未打开App）时 可以通过第三方手机操作系统的辅助通道将消息通过手机通知栏的方式投递下去

未读消息数量的统计 用户总维度 单个session的维度
具体的未读数存储可以是在IM服务端（如QQ、微博），也可以是在接收方的本地端上存储（微信），一般来说，需要支持“消息的多终端漫游”的应用需要在IM服务端进行未读存储，不需要支持“消息的多终端漫游”可以选择本地存储即可。
对于在IM服务端存储消息未读数的分布式场景，如何保证这两个未读数的一致性也是一个比较有意思的事情



## Architecture




## 消息协议

消息协议设计


协议从纵向可以分成三个层次: 应用层、安全层、传输层

应用层 协议有两种 文本协议和二进制协议 当前应用场景首选二进制协议 业界用 protobuf




安全层
- TLS/SSL 加密效果好 证书管理复杂
- 固定加密 客户端和服务端约定好密钥和加密算法
- 一人一密 通信前客户端申请请求密钥 服务端用用户特有属性生成密钥下发
- 一次一密 会话级别加密 接近于 TLS

加密消耗CPU 资源

网关层对数据包进行TLS3.0协议协商握手 加解密操作 可以使用GPU进行加解密



传输层

TCP 和 UDP 的对比


开源协议

小团队 XMPP 或者 MQTT + HTTP 短连接实现 反之需要自己设计实现私有协议 + Protobuf

- IMPP
- XMPP
- SIMPLE
- MQTT 扩展性差 需要定制化改造以支持时序性
- websocket
- 私有协议 + Protobuf





## Implementations

业界 IM：腾讯 QQ、腾讯微信、网易云通讯、抖音 IM、钉钉 IM、脉脉 IM、支付宝 IM








群聊消息

基于传统的 IM 架构技术，尤其在群内聊天或者分享，每条消息按照群内人数进行写扩散，按照主互动 500 人群规模来计算，平均群大小 320+，1:N 的写入必然导致写入 DB 的 RT 以及存储压力，
实际参与互动分享的用户在峰值的时候远大于这部分互动分享和聊天消息流量，其次集群的写入不可能完全给 IM 聊天消息，还有其它的营销活动、交易、物流等通知类型的消息
基于“写扩散”架构，在高并发互动场景下遇到了瓶颈，导致消息大量的延迟下推，影响最终用户体验
针对群内的消息可以分为“非个性化消息”和“个性化消息”，所谓“非个性化消息”就是大家看到都是一样的，应该只需要写一条数据，群内成员可以共享取这条数据，所谓“个性化消息”，指定某个成员发送的单边消息，譬如进群欢迎语等
在群内，99.99%都是“非个性化消息”，也就是可以从 1:N 压缩到 1:1 写入，基于这个理论假设，需要考虑“读扩散”让每个用户的消息从对应的“群会话消息队列同步“数据，而不是从”用户队列同步“数据，其中同步队列围绕队列 offset 偏移量进行，通过队列的自增 syncId 保证有序，每个客户端维护相应的队列的同步位点，采取“客户端存储位点的去中心化“方案，实现”下行消息的推拉“结合，通过队列位点 syncId 进行比对，如果服务端消息队列 syncId-客户端队列 syncId=1，表示云端消息无空洞，否则携带客户端的队列和对应的 syncId 到云端重新同步区间数据，实现最终一致性。
市面上 APP80%都具备 IM 聊天能力，均采取写扩散简单模式进行云端消息同步

写扩散技术
优点：
 整体架构简洁，方案简单，维护用户同步队列实现数据同步机制。
 无论是单聊还是群聊等会话消息，写入用户维度同步队列，集中获取同步数据。 推和拉情况下，存储模型和数据处理简单，且天然支持个性化数据
缺点：
 群会话消息，天然存在 1:N 写入扩散比，存储压力 N 倍压力，在线用户收到消息延迟增大。
 多个群内消息队列混合在同步队列，无优先级处理能力，无法针对互动群等做隔离。

读扩散技术
优点：
 降低写扩散 N 倍的 DB 存储压力，减少下行在线用户端到端扩散的 RT 时间。 提升消息上行集群整体的吞吐量，用户体验更流畅。
 端到端实现会话级别的同步优先级队列，实现差异化服务。
缺点：
 整体架构偏复杂，需要维护每个动态会话消息同步队列，端侧需要实时感知新增的动态同步队列。
 客户端冷启动需要分散读取多个会话消息同步队列数据，对于存储会带来读 QPS 压力

直播间
直播场景大直播间和小直播间的差距极大
对于大直播间可以提前做好直播分片
针对可靠消息（红包、优惠、宝贝卡片）等进行持久化存储，利用多次消息下推携带机制，同时兼顾端侧拉取机制，保证消息最终一致性且在 3 秒以内到达端侧


## 闲鱼

与一般IM会话模型不同的是，闲鱼会话以商品为主体，“人＋人+商品”为要素构建会话。
因会话模型的差异，淘系已有的消息系统，短期内无法满足业务需求，而闲鱼完全自建消息系统耗时巨大
为了保障业务高效上线，技术选型上最大化复用已有系统能力，避免重复造轮子。

所以，我们的技术方案是：
1. 数据模型与底层存储依赖淘系私信体系进行建设；
2. 消息数据获取上，客户端全量从服务端拉取消息数据；
3. 通讯协议使用来往SDK及mtop。


MobileMSDK



## Links



