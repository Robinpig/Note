## Introduction

即时消息（Instant Messaging，IM），又称实时消息，是一个可以支持在线交流的常见功能场景
IM并不是一门仅限于聊天、社交的技术 包括聊天、直播、在线客服、物联网等这些业务领域在内，所有需要“实时互动”“高实时性”的场景，都需要IM技术

IM 应用场景和业务功能
- QQ、微信等聊天类场景： 即时通讯
- 豆瓣、知乎、小红书等社区类场景：点对点、群组聊天
- yy、抖音等直播类场景：直播互动、实时弹幕
- 小米、华为、海康智能家居IoT场景：实时监控、远程控制
- 盛大、畅游等游戏类场景： 多人互动
- 滴滴、Uber等交通类场景： 位置共享
- 新东方、网易云课堂等教学类场景： 在线白板


IM并不是一项全新的技术，而是众多前后端技术的综合体，并因自身使用场景的不同，在一些技术点上有更多侧重


我第一次接触IM系统并不是和“人”相关的场景，当时就职的公司做的是一个类似“物联网”的油罐车实时追踪控制系统，一是通过GPS实时跟踪油罐车的位置，判断是否按常规路线行进，一是在油罐车到达目的地之后，通过系统远程控制开锁。
所以，这里的交互实际是“车”和“系统”的互动，当然这个系统实现上并没有多大的技术挑战，除了GPS的漂移纠偏带来了一些小小的困扰，最大的挑战莫过于调试的时候需要现场跟车调试多地奔波。
再后来，由于工作的变动，我逐渐接触到IM系统中一些高并发的业务场景，千万级实时在线用户，百亿级消息下推量，突发热点的直线峰值等。一步一步地踩坑和重构，除了感受到压力之外，对IM系统也有了更深层次的理解。
记得几年前，由于消息图片服务稳定性不好，图片消息的渲染比较慢，用户体验不好。
而且，由于图片流和文本流在同一个TCP通道，TCP的阻塞有时还会影响文本消息的收发。
所以后来我们对通道进行了一次拆分，把图片、文件等二进制流拆到一个独立通道，核心通道只推缩略图流，大幅减轻了通道压力，提升了核心链路的稳定性。
同时，独立的通道也缩短了客户端到文件流的链路，这样也提升了图片的访问性能。
但后来视频功能上线后，我们发现视频的PSR1（1秒内播放成功率）比较低，原因是视频文件一般比较大，为避免通道阻塞，不会通过消息收发的核心通道来推送。
所以之前的策略是：通过消息通道只下推视频的ID，用户真正点击播放时才从服务端下载，这种模式虽然解决了通道阻塞的问题，但“播放时再下载”容易出现卡顿的情况。
因此，针对视频类消息，我们增加了一个notify-pull模式，当客户端收到一条视频类消息的通知时，会再向服务器发起一个短连接的拉取请求，缓冲前N秒的数据。
这样等用户点击播放时，基本就能秒播了，较大地提升了视频消息播放的PSR1（1秒内播放成功率）。

我们要打造一套“实时、安全、稳定”的IM系统，我们需要深入思考很多个地方，尤其是作为整个实时互动业务的基础设施，扩展性、可用性、安全性等方面都需要有较高的保障。
比如下面几种情况。
- 某个明星忽然开直播了，在线用户数和消息数瞬间暴涨，该如何应对？
- 弱网情况下，怎么解决消息收发失败的问题，提升消息到达率？
- 如何避免敏感聊天内容由于网络劫持而泄露？



很多应用场景中都会将IM 看作是一个可以融入到各种业务系统中提供实时交互的模块


核心场景 A客户端发送消息到服务器 服务器发送下行消息到客户端B

基本要求: 可达有序 不重不漏（QoS2 + 时序性）
-消息要及时可达
-消息不重复遗漏
-消息时序 接收端消息保证是按发送时间排序



即时消息系统中，消息作为互动的载体，是必不可少的要素之一。
一般来说，大部分即时消息系统为了便于查看历史消息或者用于暂存离线消息，都需要对消息进行服务端存储
以点对点消息为例 收发双方的历史消息都是相互独立的 首先是一张消息内容表，然后是消息收发双发各自会在索引表中存储一条记录

一般 IM 系统还需要一个联系人列表 用于互动双方快速查找到需要聊天的对象 联系人列表还会携带双方最近一条记录用于展示 与历史消息存储的索引表不同 消息联系人表查询是用户全局维度
索引表通常是插入操作 消息联系人表是更新操作


消息接收通道

IM服务端的网关需要和消息接收者设备之间维护一条TCP长链接（或者是websocket长链接）通过长链接投递消息 当接收方不在线（网络不通、未打开App）时 可以通过第三方手机操作系统的辅助通道将消息通过手机通知栏的方式投递下去

未读消息数量的统计 用户总维度 单个session的维度
具体的未读数存储可以是在IM服务端（如QQ、微博），也可以是在接收方的本地端上存储（微信），一般来说，需要支持“消息的多终端漫游”的应用需要在IM服务端进行未读存储，不需要支持“消息的多终端漫游”可以选择本地存储即可。
对于在IM服务端存储消息未读数的分布式场景，如何保证这两个未读数的一致性也是一个比较有意思的事情



## Architecture




## 消息协议

消息协议设计


协议从纵向可以分成三个层次: 应用层、安全层、传输层

应用层 协议有两种 文本协议和二进制协议 当前应用场景首选二进制协议 业界用 protobuf




安全层
- TLS/SSL 加密效果好 证书管理复杂
- 固定加密 客户端和服务端约定好密钥和加密算法
- 一人一密 通信前客户端申请请求密钥 服务端用用户特有属性生成密钥下发
- 一次一密 会话级别加密 接近于 TLS

加密消耗CPU 资源

网关层对数据包进行TLS3.0协议协商握手 加解密操作 可以使用GPU进行加解密



传输层

TCP 和 UDP 的对比


开源协议

小团队 XMPP 或者 MQTT + HTTP 短连接实现 反之需要自己设计实现私有协议 + Protobuf

- IMPP
- XMPP
- SIMPLE
- MQTT 扩展性差 需要定制化改造以支持时序性
- websocket
- 私有协议 + Protobuf









## 时序性

对于聊天、直播互动等即时消息的场景，保持消息的时序一致性能避免发送方的意见表述和接收方的语义理解出现偏差的情况。

对于如何保持消息的时序一致性的关键点在于需要找到一个时序基准来标识每一条消息的顺序。这个时序基准可以通过全局的序号生成器来确定



使用“全局序号生成器”发出的序号，来作为消息排序的“时序基准”，能解决每一条消息没有标准“生产日期”的问题。但如果是面向高并发和需要保证高可用的场景，还需要考虑这个“全局序号生成器”的可用性问题。

首先，类似Redis的原子自增和DB的自增id，都要求在主库上来执行“取号”操作，而主库基本都是单点部署，在可用性上的保障会相对较差，另外，针对高并发的取号操作这个单点的主库可能容易出现性能瓶颈。

而采用类似snowflake算法的时间相关的分布式“序号生成器”，虽然在发号性能上一般问题不大，但也存在一些问题。

一个是发出的号携带的时间精度有限，一般能到秒级或者毫秒级，比如微博的ID生成器就是精确到秒级的，另外由于这种服务大多都是集群化部署，携带的时间采用的服务器时间，也存在时钟不一致的问题（虽然时钟同步上比控制大量的IM服务器也相对容易一些）



从业务层面考虑，对于群聊和多点登录这种场景，没有必要保证全局的跨多个群的绝对时序性，只需要保证某一个群的消息有序即可。

这样的话，如果可以针对每一个群有独立一个“ID生成器”，能通过哈希规则把压力分散到多个主库实例上，大量降低多群共用一个“ID生成器”的并发压力。

对于大部分即时消息业务来说，产品层面可以接受消息时序上存在一定的细微误差，比如同一秒收到同一个群的多条消息，业务上是可以接受这一秒的多条消息，未严格按照“接收时的顺序”来排序的，实际上，这种细微误差对于用户来说，基本也是无感知的。



从之前微信对外的分享，我们可以了解到：微信的聊天和朋友圈的消息时序也是通过一个“递增”的版本号服务来进行实现的。不过这个版本号是每个用户独立空间的，保证递增，不保证连续。

微博的消息箱则是依赖“分布式的时间相关的ID生成器”来对私信、群聊等业务进行排序，目前的精度能保证秒间有序



有了“时序基准”，是不是就能确保消息能按照“既定顺序”到达接收方呢？答案是并不一定能做到。原因在于下面两点。

1. IM服务器都是集群化部署，每台服务器的机器性能存在差异，因此处理效率有差别，并不能保证先到的消息一定可以先推送到接收方，比如有的服务器处理得慢，或者刚好碰到一次GC，导致它接收的更早消息，反而比其他处理更快的机器更晚推送出去。
2. IM服务端接收到发送方的消息后，之后相应的处理一般都是多线程进行处理的，比如“取序号”“暂存消息”“查询接收方连接信息”等，由于多线程处理流程，并不能保证先取到序号的消息能先到达接收方，这样的话对于多个接收方看到的消息顺序可能是不一致的。

所以一般还需要端上能支持对消息的“本地整流”



### 消息服务端包内整流

虽然大部分情况下，聊天、直播互动等即时消息业务能接受“小误差的消息乱序”，但某些特定场景下，可能需要IM服务能保证绝对的时序。

比如发送方的某一个行为同时触发了多条消息，而且这多条消息在业务层面需要严格按照触发的时序来投递。

一个例子：用户A给用户B发送最后一条分手消息同时勾上了“取关对方”的选项，这个时候可能会同时产生“发消息”和“取关”两条消息，如果服务端处理时，把“取关”这条信令消息先做了处理，就可能导致那条“发出的消息”由于“取关”了，发送失败的情况。

对于这种情况，我们一般可以调整实现方式，在发送方对多个请求进行业务层合并，多条消息合并成一条；也可以让发送方通过单发送线程和单TCP连接能保证两条消息有序到达。

但即使IM服务端接收时有序，由于多线程处理的原因，真正处理或者下推时还是可能出现时序错乱的问题，解决这种“需要保证多条消息绝对有序性”可以通过IM服务端包内整流来实现。

比如：我们在实现离线推送时，在网关机启动后会自动订阅一个本IP的Topic，当用户上线时，网关机会告知业务层用户有上线操作，这时业务层会把这个用户的多条离线消息pub给这个用户连接的那个网关机订阅的Topic，当网关机收到这些消息后，再通过长连接推送给用户，整个过程大概是下图这样的。
![img](https://static001.geekbang.org/resource/image/85/5f/85d6aa4b551a21c34ed6501ecf19445f.png?wh=1024%2A768)

但是很多时候会出现Redis队列组件的Sharding和网关机多线程消费处理导致乱序的



然后再说一下离线推送服务端整流的过程：

![img](https://static001.geekbang.org/resource/image/f9/b8/f97ce351151a86cd7cfb06d7f58513b8.png?wh=1024%2A768)

- 首先，生产者为每个消息包生成一个packageID，为包内的每条消息加个有序自增的seqId。
- 其次，消费者根据每条消息的packageID和seqID进行整流，最终执行模块只有在**一定超时时间内完整有序**地收到所有消息才执行最终操作，否则将根据业务需要触发重试或者直接放弃操作。通过服务端整流，服务端包内整流大概就是图中这个样子，我们要做的是在最终服务器取到TCP连接后下推的时候，根据包的ID，对一定时间内的消息做一个整流和排序，这样即使服务端处理多条消息时出现乱序，仍然可以在最终推送给客户端时整流为有序的。

### 消息接收端整流

携带不同序号的消息到达接收端后，可能会出现“先产生的消息后到”“后产生的消息先到”等问题，消息接收端的整流就是解决这样的一个问题的。

消息客户端本地整流的方式可以根据具体业务的特点来实现，目前业界比较常见的实现方式比较简单，步骤如下：

1. 下推消息时，连同消息和序号一起推送给接收方；
2. 接收方收到消息后进行判定，如果当前消息序号大于前一条消息的序号就将当前消息追加在会话里；
3. 否则继续往前查找倒数第二条、第三条等，一直查找到恰好小于当前推送消息的那条消息，然后插入在其后展示

## Implementations

业界 IM：腾讯 QQ、腾讯微信、网易云通讯、抖音 IM、钉钉 IM、脉脉 IM、支付宝 IM








群聊消息

基于传统的 IM 架构技术，尤其在群内聊天或者分享，每条消息按照群内人数进行写扩散，按照主互动 500 人群规模来计算，平均群大小 320+，1:N 的写入必然导致写入 DB 的 RT 以及存储压力，
实际参与互动分享的用户在峰值的时候远大于这部分互动分享和聊天消息流量，其次集群的写入不可能完全给 IM 聊天消息，还有其它的营销活动、交易、物流等通知类型的消息
基于“写扩散”架构，在高并发互动场景下遇到了瓶颈，导致消息大量的延迟下推，影响最终用户体验
针对群内的消息可以分为“非个性化消息”和“个性化消息”，所谓“非个性化消息”就是大家看到都是一样的，应该只需要写一条数据，群内成员可以共享取这条数据，所谓“个性化消息”，指定某个成员发送的单边消息，譬如进群欢迎语等
在群内，99.99%都是“非个性化消息”，也就是可以从 1:N 压缩到 1:1 写入，基于这个理论假设，需要考虑“读扩散”让每个用户的消息从对应的“群会话消息队列同步“数据，而不是从”用户队列同步“数据，其中同步队列围绕队列 offset 偏移量进行，通过队列的自增 syncId 保证有序，每个客户端维护相应的队列的同步位点，采取“客户端存储位点的去中心化“方案，实现”下行消息的推拉“结合，通过队列位点 syncId 进行比对，如果服务端消息队列 syncId-客户端队列 syncId=1，表示云端消息无空洞，否则携带客户端的队列和对应的 syncId 到云端重新同步区间数据，实现最终一致性。
市面上 APP80%都具备 IM 聊天能力，均采取写扩散简单模式进行云端消息同步

写扩散技术
优点：
 整体架构简洁，方案简单，维护用户同步队列实现数据同步机制。
 无论是单聊还是群聊等会话消息，写入用户维度同步队列，集中获取同步数据。 推和拉情况下，存储模型和数据处理简单，且天然支持个性化数据
缺点：
 群会话消息，天然存在 1:N 写入扩散比，存储压力 N 倍压力，在线用户收到消息延迟增大。
 多个群内消息队列混合在同步队列，无优先级处理能力，无法针对互动群等做隔离。

读扩散技术
优点：
 降低写扩散 N 倍的 DB 存储压力，减少下行在线用户端到端扩散的 RT 时间。 提升消息上行集群整体的吞吐量，用户体验更流畅。
 端到端实现会话级别的同步优先级队列，实现差异化服务。
缺点：
 整体架构偏复杂，需要维护每个动态会话消息同步队列，端侧需要实时感知新增的动态同步队列。
 客户端冷启动需要分散读取多个会话消息同步队列数据，对于存储会带来读 QPS 压力

直播间
直播场景大直播间和小直播间的差距极大
对于大直播间可以提前做好直播分片
针对可靠消息（红包、优惠、宝贝卡片）等进行持久化存储，利用多次消息下推携带机制，同时兼顾端侧拉取机制，保证消息最终一致性且在 3 秒以内到达端侧


## 闲鱼

与一般IM会话模型不同的是，闲鱼会话以商品为主体，“人＋人+商品”为要素构建会话。
因会话模型的差异，淘系已有的消息系统，短期内无法满足业务需求，而闲鱼完全自建消息系统耗时巨大
为了保障业务高效上线，技术选型上最大化复用已有系统能力，避免重复造轮子。

所以，我们的技术方案是：
1. 数据模型与底层存储依赖淘系私信体系进行建设；
2. 消息数据获取上，客户端全量从服务端拉取消息数据；
3. 通讯协议使用来往SDK及mtop。


MobileMSDK



## Links



