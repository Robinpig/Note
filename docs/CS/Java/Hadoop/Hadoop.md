## Introduction

The [Apache Hadoop](https://hadoop.apache.org/) software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models.

The project includes these modules:

- Hadoop Common: The common utilities that support the other Hadoop modules.
- Hadoop Distributed File System (HDFS™): A distributed file system that provides high-throughput access to application data.
- Hadoop YARN: A framework for job scheduling and cluster resource management.
- Hadoop MapReduce: A YARN-based system for parallel processing of large data sets.

## Architecture

The project includes these modules:

* **Hadoop Common** : The common utilities that support the other Hadoop modules.
* **Hadoop Distributed File System (HDFS™)** : A distributed file system that provides high-throughput access to application data.
* **Hadoop YARN** : A framework for job scheduling and cluster resource management.
* **Hadoop MapReduce** : A YARN-based system for parallel processing of large data sets.



## Links


## References
