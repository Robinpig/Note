## Introduction

The G1, Garbage First, garbage collector (GC) has been the default GC since JDK 9.
The Garbage-First (G1) garbage collector is a server-style garbage collector, **targeted for multiprocessor machines with large memories**.
It attempts to meet garbage collection (GC) pause time goals with high probability while achieving high throughput.
Whole-heap operations, such as global marking, are performed concurrently with the application threads. This prevents interruptions proportional to heap or live-data size.

The first focus of G1 is to provide a solution for users running applications that require large heaps with limited GC latency.
**This means heap sizes of around 6 GB or larger, and a stable and predictable pause time below 0.5 seconds.**

Applications running today with either the CMS or the with parallel compaction would benefit from switching to G1 if the application has one or more of the following traits.

- More than 50% of the Java heap is occupied with live data.
- The rate of object allocation rate or promotion varies significantly.
- The application is experiencing undesired long garbage collection or compaction pauses (longer than 0.5 to 1 second).

G1 performs parts of its work at the same time as the application runs. It trades processor resources which would otherwise be available to the application for shorter collection pauses.

This is most visible in the use of one or more garbage collection threads active while the application runs.
Thus, compared to throughput collectors, while garbage collection pauses are typically much shorter with the G1 collector, application throughput also tends to be slightly lower.

G1 is a generational, incremental, parallel, mostly concurrent, stop-the-world, and evacuating garbage collector which monitors pause-time goals in each of the stop-the-world pauses.
Similar to other collectors, G1 splits the heap into (virtual) young and old generations.
Space-reclamation efforts concentrate on the young generation where it is most efficient to do so, with occasional space-reclamation in the old generation.

Some operations are always performed in stop-the-world pauses to improve throughput.
Other operations that would take more time with the application stopped such as whole-heap operations like global marking are performed in parallel and concurrently with the application.
To keep stop-the-world pauses short for space-reclamation, G1 performs space-reclamation incrementally in steps and in parallel.
G1 achieves predictability by tracking information about previous application behavior and garbage collection pauses to build a model of the associated costs.
It uses this information to size the work done in the pauses. For example, G1 reclaims space in the most efficient areas first (that is the areas that are mostly filled with garbage, therefore the name).

G1 reclaims space mostly by using evacuation: live objects found within selected memory areas to collect are copied into new memory areas, compacting them in the process.
After an evacuation has been completed, the space previously occupied by live objects is reused for allocation by the application.

The Garbage-First collector is not a real-time collector.
It tries to meet set pause-time targets with high probability over a longer time, but not always with absolute certainty for a given pause.

## Heap Layout

G1 partitions the heap into a set of equally sized heap regions, each a contiguous range of virtual memory as shown in Figure 1.
A region is the unit of memory allocation and memory reclamation.
At any given time, each of these regions can be empty (light gray), or assigned to a particular generation, young or old. As requests for memory comes in, the memory manager hands out free regions.
The memory manager assigns them to a generation and then returns them to the application as free space into which it can allocate itself.

<div style="text-align: center;">

![zHeap Layout](img/G1-Heap-Layout.png)

</div>

<p style="text-align: center;">
Fig.1. G1 Garbage Collector Heap Layout. 
</p>

The young generation contains eden regions (red) and survivor regions (red with "S").
These regions provide the same function as the respective contiguous spaces in other collectors, with the difference that in G1 these regions are typically laid out in a noncontiguous pattern in memory.
Old regions (light blue) make up the old generation. Old generation regions may be humongous (light blue with "H") for objects that span multiple regions.

Allocation in a heap region consists of incrementing a boundary, top, between allocated and unallocated space.
One region is the current allocation region from which storage is being allocated.
Since we are mainly concerned with multiprocessors, mutator threads allocate only thread-local allocation buffers, or TLABs, directly in this heap region, using a compare-and-swap, or CAS, operation.
They then allocate objects privately within those buffers, to minimize allocation contention.
When the current allocation region is filled, a new allocation region is chosen. Empty regions are organized into a linked list to make region allocation a constant time operation.
Larger objects may be allocated directly in the current allocation region, outside of TLABs.
Objects whose size exceeds 3/4 of the heap region size, however, are termed humongous.

Each region has an associated remembered set, which indicates all locations that might contain pointers to (live) objects within the region.
Maintaining these remembered sets requires that mutator threads inform the collector when they make pointer modifications that might create inter-region pointers.
This notification uses a card table: every 512-byte card in the heap maps to a one-byte entry in the card table.
Each thread has an associated remembered set log, a current buffer or sequence of modified cards.
In addition, there is a global set of filled RS buffers.

The remembered sets themselves are sets (represented by hash tables) of cards.
Actually, because of parallelism, each region has an associated array of several such hash tables, one per parallel GC thread, to allow these threads to update remembered sets without interference.
The logical contents of the remembered set is the union of the sets represented by each of the component hash tables.
The remembered set write barrier is performed after the pointer write.
If the code performs the pointer write x.f = y, and registers rX and rY contain the object pointer values x and y respectively, then the pseudo-code for the barrier is:

```
1| rTmp := rX XOR rY
2| rTmp := rTmp >> LogOfHeapRegionSize
3| // Below is a conditional move instr
4| rTmp := (rY == NULL) then 0 else rTmp
5| if (rTmp == 0) goto filtered
6| call rs_enqueue(rX)
7| filtered:
```

This barrier uses a filtering technique mentioned briefly by Stefanovi´c et al..
If the write creates a pointer from an object to another object in the same heap region, a case we expect to be common, then it need not be recorded in a remembered set.
The exclusive-or and shifts of lines 1 and 2 means that rTmp is zero after the second line if x and y are in the same heap region.
Line 4 adds filtering of stores of null pointers.
If the store passes these filtering checks, then it creates an out-of-region pointer.
The rs enqueue routine reads the card table entry for the object head rX.
If that entry is already dirty, nothing is done.
This reduces work for multiple stores to the same card, a common case because of initializing writes.
If the card table entry is not dirty, then it is dirtied, and a pointer to the card is enqueued on the thread’s remembered set log.
If this enqueue fills the thread’s current log buffer (which holds 256 elements by default), then that buffer is put in the global set of filled buffers, and a new empty buffer is allocated.

The concurrent remembered set thread waits (on a condition variable) for the size of the filled RS buffer set to reach a configurable initiating threshold (the default is 5 buffers).
The remembered set thread processes the filled buffers as a queue, until the length of the queue decreases to 1/4 of the initiating threshold.
For each buffer, it processes each card table pointer entry.
Some cards are hot:they contain locations that are written to frequently.
To avoid processing hot cards repeatedly, we try to identify the hottest cards, and defer their processing until the next evacuation pause.
We accomplish this with a second card table that records the number of times the card has been dirtied since the last evacuation pause (during which this table, like the card table proper, is cleared).
When we process a card we increment its count in this table.
If the count exceeds a hotness threshold (default 4), then the card is added to circular buffer called the hot queue (of default size 1 K).
This queue is processed like a log buffer at the start of each evacuation pause, so it is empty at the end.
If the circular buffer is full, then a card is evicted from the other end and processed.

Thus, the concurrent remembered set thread processes a card if it has not yet reached the hotness threshold, or if it is evicted from the hot queue.
To process a card, the thread first resets the corresponding card table entry to the clean value, so that any concurrent modifications to objects on the card will redirty and re-enqueue the card.
It then examines the pointer fields of all the objects whose modification may have dirtied the card, looking for pointers outside the containing heap region.
If such a pointer is found, the card is inserted into the remembered set of the referenced region.

We use only a single concurrent remembered set thread, to introduce parallelism when idle processors exist.
However, if this thread is not sufficient to service the rate of mutation, the filled RS buffer set will grow too large.
We limit the size of this set; mutator threads attempting to add further buffers perform the remembered set processing themselves.

-XX:ParallelGCThreads

Default  2048 Region

use -XX:+G1HeapRegionSize could define but suggest default

Young default 5% of heap size and suggest less than 60%

-XX:G1NewSizePercent -XX:G1MaxNewSizePercent

Eden : survivor1: survivor2 = 8: 1: 1 still

Humongous over half of region size

可预测停顿 

-XX:MaxGCPauseMillis=value

### Heap Sizing

G1 respects standard rules when resizing the Java heap, using `-XX:InitialHeapSize` as the minimum Java heap size, `-XX:MaxHeapSize` as the maximum Java heap size,
`-XX:MinHeapFreeRatio` for the minimum percentage of free memory, `-XX:MaxHeapFreeRatio` for determining the maximum percentage of free memory after resizing.
The G1 collector considers to resize the Java heap according to these options during a the Remark and the Full GC pauses only.
This process may release memory to or allocate memory from the operating system.

Heap expansion occurs within the collection pause, while memory release occurs after the pause concurrent to the application

#### Young-Only Phase Generation Sizing

G1 always sizes the young generation at the end of a normal young collection for the next mutator phase.
This way, G1 can meet the pause time goals that were set using -XX:MaxGCPauseTimeMillis and -XX:GCPauseIntervalMillis based on long-term observations of actual pause time.
This calculation takes into account how long it took young generations of similar size to evacuate.
This includes information like how many objects had to be copied during collection, and how interconnected these objects had been.

The -XX:GCPauseIntervalMillis and -XX:MaxGCPauseTimeMillis options define a minimum mutator utilization (MMU).
G1 will try for every possible time range of -XX:GCPauseIntervalMillis at most use -XX:MaxGCPauseTimeMillis milliseconds for garbage collection pauses.

If not otherwise constrained, then G1 adaptively sizes the young generation size between the values that -XX:G1NewSizePercent and -XX:G1MaxNewSizePercent determine to meet pause-time.
See Garbage-First Garbage Collector Tuning  for more information about how to fix long pauses.

Alternatively, -XX:NewSize in combination with -XX:MaxNewSize may be used to set minimum and maximum young generation size respectively.

> [!NOTE]
>
> Only specifying one of these latter options fixes young generation size to exactly the value passed with -XX:NewSize and -XX:MaxNewSize respectively. This disables pause time control.

#### Space-Reclamation Phase Generation Sizing

During the space-reclamation phase, G1 tries to maximize the amount of space that is reclaimed in the old generation in a single garbage collection pause. The size of the young generation is typically set to the minimum allowed, typically as determined by -XX:G1NewSizePercent, but also considering the MMU specification.

At the start of every mixed collection in this phase, G1 selects a set of regions from the collection set candidates to add to the collection set. This additional set of old generation regions consists of three parts:

- A minimum set of old generation regions to ensure evacuation progress.
  This set of old generation regions is determined by the number of regions in the collection set candidates divided by the length of the Space-Reclamation phase as determined by -XX:G1MixedGCCountTarget.
- Additional old generation regions from the collection set candidates if G1 predicts that after collecting the minimum set there will be time left.
  Old generation regions are added until 80% of the remaining time is predicted to be used.
- A set of optional collection set regions that G1 evacuates incrementally after the other two parts have been evacuated and there is time left in this pause.

The first two sets of regions are collected in an initial collection pass, with additional regions from the optional collection set fit into the remaining pause time.
This method ensures space reclamation progress while improving the probability to keep pause time and minimal overhead due to management of the optional collection set.

The Space-Reclamation phase ends when there are no more regions in the collection set candidate regions set.

#### Periodic Garbage Collections

If there is no garbage collection for a long time because of application inactivity, the VM may hold on to a large amount of unused memory for a long time that could be used elsewhere.
To avoid this, G1 can be forced to do regular garbage collection using the -XX:G1PeriodicGCInterval option.
This option determines a minimum interval in ms at which G1 considers performing a garbage collection.
If this amount of time passed since any previous garbage collection pause and there is no concurrent cycle in progress, G1 triggers additional garbage collections with the following possible effects:

During the Young-Only phase: G1 starts a concurrent marking using a Concurrent Start pause or, if -XX:-G1PeriodicGCInvokesConcurrent has been specified, a Full GC.
During the Space Reclamation phase: G1 continues the space reclamation phase triggering the garbage collection pause type appropriate to current progress.
The -XX:G1PeriodicGCSystemLoadThreshold option may be used to refine whether a garbage collection is triggered: if the average one-minute system load value as returned by the getloadavg() call on the JVM host system (for example, a container) is above this value, no periodic garbage collection will be run.

#### Determining Initiating Heap Occupancy

The Initiating Heap Occupancy Percent (IHOP) is the threshold at which a Concurrent Start collection is triggered and it is defined as a percentage of the old generation size.

G1 by default automatically determines an optimal IHOP by observing how long marking takes and how much memory is typically allocated in the old generation during marking cycles.
This feature is called Adaptive IHOP.
If this feature is active, then the option -XX:InitiatingHeapOccupancyPercent determines the initial value as a percentage of the size of the current old generation as long as there aren't enough observations to make a good prediction of the Initiating Heap Occupancy threshold.
Turn off this behavior of G1 using the option-XX:-G1UseAdaptiveIHOP. In this case, the value of -XX:InitiatingHeapOccupancyPercent always determines this threshold.

Internally, Adaptive IHOP tries to set the Initiating Heap Occupancy so that the first mixed garbage collection of the space-reclamation phase starts when the old generation occupancy is at a current maximum old generation size minus the value of -XX:G1HeapReservePercent as the extra buffer.

#### Behavior in Very Tight Heap Situations

When the application keeps alive so much memory so that an evacuation can't find enough space to copy to, an evacuation failure occurs.
Evacuation failure means that G1 tries to complete the current garbage collection by keeping any objects that have already been moved in their new location, and not copying any not yet moved objects, only adjusting references between the object.
Evacuation failure may incur some additional overhead, but generally should be as fast as other young collections.
After this garbage collection with the evacuation failure, G1 will resume the application as normal without any other measures.
G1 assumes that the evacuation failure occurred close to the end of the garbage collection; that is, most objects were already moved and there is enough space left to continue running the application until marking completes and space-reclamation starts.

If this assumption doesn’t hold, then G1 will eventually schedule a Full GC. This type of collection performs in-place compaction of the entire heap. This might be very slow.

G1 tries to avoid evacuation failure during young collections by scheduling Preventive young collections.
The assumption is that an additional regular young collection that does not incur an evacuation failure might free enough memory in the old generation by reclaiming humongous regions to not incur the overhead of such a garbage collection at all.
Preventive young collections may be turned off using the -XX:-UsePreventiveGC option.

```cpp

  // Minimum region size; we won't go lower than that.
  // We might want to decrease this in the future, to deal with small
  // heaps a bit more efficiently.
  static const size_t MIN_REGION_SIZE = 1024 * 1024;

  // Maximum region size determined ergonomically.
  static const size_t MAX_ERGONOMICS_SIZE = 32 * 1024 * 1024;
  // Maximum region size; we don't go higher than that. There's a good
  // reason for having an upper bound. We don't want regions to get too
  // large, otherwise cleanup's effectiveness would decrease as there
  // will be fewer opportunities to find totally empty regions after
  // marking.
  static const size_t MAX_REGION_SIZE = 512 * 1024 * 1024;

  // The automatic region size calculation will try to have around this
  // many regions in the heap.
  static const size_t TARGET_REGION_NUMBER = 2048;

void HeapRegion::setup_heap_region_size(size_t max_heap_size) {
  size_t region_size = G1HeapRegionSize;
  // G1HeapRegionSize = 0 means decide ergonomically.
  if (region_size == 0) {
    region_size = clamp(max_heap_size / HeapRegionBounds::target_number(),
                        HeapRegionBounds::min_size(),
                        HeapRegionBounds::max_ergonomics_size());
  }

  // Make sure region size is a power of 2. Rounding up since this
  // is beneficial in most cases.
  region_size = round_up_power_of_2(region_size);
```

Now make sure that we don't go over or under our limits.

1MB ~ 32MB

```cpp
  region_size = clamp(region_size, HeapRegionBounds::min_size(), HeapRegionBounds::max_size());

  // Calculate the log for the region size.
  int region_size_log = log2i_exact(region_size);

  // Now, set up the globals.
  guarantee(LogOfHRGrainBytes == 0, "we should only set it once");
  LogOfHRGrainBytes = region_size_log;

  guarantee(GrainBytes == 0, "we should only set it once");
  GrainBytes = region_size;

  guarantee(GrainWords == 0, "we should only set it once");
  GrainWords = GrainBytes >> LogHeapWordSize;

  guarantee(CardsPerRegion == 0, "we should only set it once");
  CardsPerRegion = GrainBytes >> G1CardTable::card_shift();

  LogCardsPerRegion = log2i(CardsPerRegion);

  if (G1HeapRegionSize != GrainBytes) {
    FLAG_SET_ERGO(G1HeapRegionSize, GrainBytes);
  }
}
```

### young size

- -XX:NewSize and -XX:MaxNewSize override -XX:NewRatio

```cpp

G1YoungGenSizer::G1YoungGenSizer() : _sizer_kind(SizerDefaults),
  _use_adaptive_sizing(true), _min_desired_young_length(0), _max_desired_young_length(0) {

  if (FLAG_IS_CMDLINE(NewRatio)) {
    if (FLAG_IS_CMDLINE(NewSize) || FLAG_IS_CMDLINE(MaxNewSize)) {
      log_warning(gc, ergo)("-XX:NewSize and -XX:MaxNewSize override -XX:NewRatio");
    } else {
      _sizer_kind = SizerNewRatio;
      _use_adaptive_sizing = false;
      return;
    }
  }

  if (NewSize > MaxNewSize) {
    if (FLAG_IS_CMDLINE(MaxNewSize)) {
      log_warning(gc, ergo)("NewSize (" SIZE_FORMAT "k) is greater than the MaxNewSize (" SIZE_FORMAT "k). "
                            "A new max generation size of " SIZE_FORMAT "k will be used.",
                            NewSize/K, MaxNewSize/K, NewSize/K);
    }
    FLAG_SET_ERGO(MaxNewSize, NewSize);
  }

  if (FLAG_IS_CMDLINE(NewSize)) {
    _min_desired_young_length = MAX2((uint) (NewSize / HeapRegion::GrainBytes),
                                     1U);
    if (FLAG_IS_CMDLINE(MaxNewSize)) {
      _max_desired_young_length =
                             MAX2((uint) (MaxNewSize / HeapRegion::GrainBytes),
                                  1U);
      _sizer_kind = SizerMaxAndNewSize;
      _use_adaptive_sizing = _min_desired_young_length != _max_desired_young_length;
    } else {
      _sizer_kind = SizerNewSizeOnly;
    }
  } else if (FLAG_IS_CMDLINE(MaxNewSize)) {
    _max_desired_young_length =
                             MAX2((uint) (MaxNewSize / HeapRegion::GrainBytes),
                                  1U);
    _sizer_kind = SizerMaxNewSizeOnly;
  }
}
```

### Humongous Objects

Humongous objects are objects larger or equal the size of half a region.
The current region size is determined ergonomically as described in the Ergonomic Defaults for G1 GC section, unless set using the -XX:G1HeapRegionSize option.

These humongous objects are sometimes treated in special ways:

- Every humongous object gets allocated as a sequence of contiguous regions in the old generation.
  The start of the object itself is always located at the start of the first region in that sequence.
  Any leftover space in the last region of the sequence will be lost for allocation until the entire object is reclaimed.
- Generally, humongous objects can be reclaimed only at the end of marking during the Cleanup pause, or during Full GC if they became unreachable.
  There is, however, a special provision for humongous objects for arrays of primitive types for example, bool, all kinds of integers, and floating point values.
  G1 opportunistically tries to reclaim humongous objects if they are not referenced by many objects at any kind of garbage collection pause.
  This behavior is enabled by default but you can disable it with the option -XX:G1EagerReclaimHumongousObjects.
- Allocations of humongous objects may cause garbage collection pauses to occur prematurely.
  G1 checks the Initiating Heap Occupancy threshold at every humongous object allocation and may force an initial mark young collection immediately, if current occupancy exceeds that threshold.
- The humongous objects never move, not even during a Full GC.
  This can cause premature slow Full GCs or unexpected out-of-memory conditions with lots of free space left due to fragmentation of the region space.

## Garbage Collection Cycle

On a high level, the G1 collector alternates between two phases.
The young-only phase contains garbage collections that fill up the currently available memory with objects in the old generation gradually.
The space-reclamation phase is where G1 reclaims space in the old generation incrementally, in addition to handling the young generation.
Then the cycle restarts with a young-only phase.

<div style="text-align: center;">

![Garbage Collection Cycle](img/G1-GC-Cycle.png)

</div>

<p style="text-align: center;">
Fig.2. Garbage Collection Cycle. 
</p>

The following list describes the phases, their pauses and the transition between the phases of the G1 garbage collection cycle in detail:

1. Young-only phase: This phase starts with a few Normal young collections that promote objects into the old generation.
   The transition between the young-only phase and the space-reclamation phase starts when the old generation occupancy reaches a certain threshold, the Initiating Heap Occupancy threshold.
   At this time, G1 schedules a Concurrent Start young collection instead of a Normal young collection.

   - Concurrent Start : This type of collection starts the marking process in addition to performing a Normal young collection.
     Concurrent marking determines all currently reachable (live) objects in the old generation regions to be kept for the following space-reclamation phase.
     While collection marking hasn’t completely finished, Normal young collections may occur.
     Marking finishes with two special stop-the-world pauses: Remark and Cleanup.
     <br>
     The Concurrent Start pause may also determine that there is no need to follow through with marking: in this case, a short concurrent mark undo phase occurs, and the Young Only phase continues.
     In this case no Remark and Cleanup pauses will occur.
   - Remark: This pause finalizes the marking itself, performs reference processing and class unloading, reclaims completely empty regions and cleans up internal data structures.
     Between Remark and Cleanup G1 calculates information to later be able to reclaim free space in selected old generation regions concurrently, which will be finalized in the Cleanup pause.
   - Cleanup: This pause determines whether a space-reclamation phase will actually follow.
     If a space-reclamation phase follows, the young-only phase completes with a single Prepare Mixed young collection.
2. Space-reclamation phase: This phase consists of multiple young collections that in addition to young generation regions, also evacuate live objects of sets of old generation regions.
   These collections are also called Mixed collections. The space-reclamation phase ends when G1 determines that evacuating more old generation regions wouldn't yield enough free space worth the effort.

After space-reclamation, the collection cycle restarts with another young-only phase.
As backup, if the application runs out of memory while gathering liveness information, G1 performs an in-place stop-the-world full heap compaction(Full GC) like other collectors.

## Garbage Collection Pauses and Collection Set

G1 performs garbage collections and space reclamation in stop-the-world pauses.
Live objects are typically copied from source regions to one or more destination regions in the heap, and existing references to these moved objects are adjusted.

For non-humongous regions, the destination region for an object is determined from the source region of that object:

- Objects of the young generation (eden and survivor regions) are copied into survivor or old regions, depending on their age.
- Objects from old regions are copied to other old regions.

Objects in humongous regions are treated differently. G1 only determines their liveness, and if they are not live, reclaims the space they occupy. Objects within humongous regions are never moved by G1.

### Remembered Set

To evacuate the collection set G1 manages a remembered set: the set of locations outside the collection set that contain references into the collection set.
When an object from the collection set moves during garbage collection, any other references to that object from outside the collection set need to be changed to point to the new location of the object.

The remembered set entries represent approximate locations to save memory: often references close together reference objects close together.
G1 logically partitions the heap into cards, by default 512 byte sized areas. Remembered set entries are compressed indexes of these cards.

G1 initially manages this remembered set on a per-region basis: every region contains a per-region remembered set, the set of locations with potential references into this region.
During garbage collection, the remembered set for the entire collection set is generated from these.

The remembered sets are created mostly lazily: between the Remark and Cleanup pause G1 rebuilds the remembered set of all collection set candidate regions.
Other than that G1 always maintains remembered sets for young generation region as they are collected at every collection and by default some humongous objects for eager reclaim.


RSet 记录的是引用关系，G1使用Point-In的方式，记录其他区域对本区的引用关系，如下所示：
1. 为什么不需要新生代到老生代的引用
1. 因为Mix GC肯定会先GC Young区，不需要考虑Young区引用Old区的问题
1. 为什么需要Old区到Old区的引用
1. 因为Mix回收不是整个Old区

| 引用关系	| 是否需要RSet |
| --- | --- |
| 分区内部	| 不需要 |
| 新生代到新生代 |	不需要 |
| 新生代到老年代 |	不需要 |
| 老年代到新生代 |	需要 |
| 老年代到老年代 |	需要 |

不需要回收的原因如下
RH -> RH - GC是以Region为最小单位，回收时会扫描整个Region，
也就是Region内每个对象都会扫描到，谁引用谁自然知道了，无需额外空间记录
在处理RSet时过滤

YRH -> YRH - G1提供了三种GC-YGC、Mixed GC、Full GC,每种GC都会全量回收新生代Region，
新生代中每个Region都会被整个扫描，代间引用就不用记了
在写屏障时过滤

YRH -> ORH - YGC:只回收新生代，与老年代无关，不用记;
Mixed GC : 以新生代为根，根可达分析的时候就找到老生代了,不用记
Full GC : 整堆回收,还记啥了
在写屏障时过滤
RSet实现
稀疏哈希表 - SparsePRT
细粒度 - PerRegionTable
粗粒度位图 - BitMap
卡表就是将Region进一步划分，将Region划分为若干个物理连续的512Byte的card page - 卡页，这样每个Region就有一个卡表来映射Regin中的卡页，整堆有个global card table - 全局卡表 存储所有Region的卡表情况
如下图所示，假设数据如下，那么，card table、heap region、heap的关系如图所示
Heap - 堆 大小设置为 1GB
那么global card table - 全局卡表 的长度就是 1GB / 512Byte = 2097151 个
HR - Heap Region - 每个Region大小设置为 1MB
那么每个Region 的卡表长度就是 1MB / 512Byte = 2048 个RSet 的模型
稀疏Hash Table - SparsePRT
key - region address 引用对象所在Region的地址
value - card page index array - 引用对象所在Region中卡表索引值数组，

对象的引用关系如下所示
1. B、C、D 分区中的对象b、c、d都引用了A分区中的a对象
2. Point In方式记录，所以Region A 中的RSet会记录这个3个引用情况
3. key分别记录 B、C、D 分区地址
4. value 分别记录b、c、d对象所在卡页对应卡表的index
5. c对象比较大，占3个card page,所以value是数组类型，存放index的值

细粒度 - PerRegionTable
位图来映射卡表就是:
- 一个Bit代表一个card(512Byte) , 8Bit = Byte ，你看看，1:4096的比例
- 0代表这个卡表中对象没有引用
- 1代表有引用

粗粒度位图 - BitMap
粗粒度位图和细粒度位图的主要区别是：粗粒度位图中，每一个Bit代表的是一个Region，而非Card，粒度更粗。其他概念和模型基本相似，且都属于同一个对象（PerRegionTable，后文代码详解中有描述），这里略过不表。


### Collection Set

The collection set is the set of source regions to reclaim space from. Depending on the type of garbage collection, the collection set consists of different kinds of regions:

- In a Young-Only phase, the collection set consists only of regions in the young generation, and humongous regions with objects that could potentially be reclaimed.
- In the Space-Reclamation phase, the collection set consists of regions in the young generation, humongous regions with objects that could potentially be reclaimed, and some old generation regions from the set of collection set candidate regions.

The collection set candidate regions are regions that are highly likely to be collected in the space reclamation phase.
G1 selects them during the Remark pause according to how much live data they contain and their connectivity with other regions.
Regions with little live data (lots of free space) are preferred over regions that are mostly live, and regions with little connectivity over regions with high connectivity, as the effort to collect these more "efficient" regions is smaller.
G1 will drop regions that do not contribute much to free memory gain from the collection set candidate regions.
This includes all regions where the amount of space that can be reclaimed is less than -XX:G1HeapWastePercent percent of the current heap size.
G1 will not collect these regions later this space reclamation phase. default 5, suggest 2

Between the Remark and Cleanup pause G1 proceeds to prepare them for later collection, with the Cleanup pause finishing the work and sorting them according to efficiency.
More efficient regions that take less time to collect and that contain more free space are preferably collected in the subsequent Mixed collections.

### Garbage Collection Process

A garbage collection consists of four phases.

- The Pre Evacuate Collection Set phase performs some preparatory work for garbage collection: disconnecting TLABs from mutator threads, selecting the collection set for this collection as described in Java Heap Sizing, and other small preparatory work.
- During Merge Heap Roots G1 creates a single unified remembered set for later easier parallel processing from the collection set regions.
  This removes many duplicates from the individual remembered sets that would otherwise be needed to be filtered out later in a more expensive way.
- The Evacuate Collection Set phase contains the bulk of the work: G1 starts moving objects starting from the roots.
  A root reference is a reference from outside the collection set, either from some VM internal data structure (external roots), code (code roots) or from the remainder of the Java heap (heap roots).
  For all roots, G1 copies the referenced object in the collection set to its destination, processes its references into the collection set as new roots until there are no more roots.
  Individual timing for these phases can be observed with -Xlog:gc+phases=debug logging in the Ext Root Scanning, Code Root Scan, Scan Heap Roots, and Object Copy sub-phases respectively.
  G1 may optionally repeat main evacuation phases for optional collection sets.
- The Post Evacuate Collection Set consists of clean-up work including reference processing and setup for the following mutator phase.

These phases correspond to the phases printed with -Xlog:gc+phases=info logging.

Young GC

Mixed GC

Full GC

-XX:InitiatingHeapOccupancyPercent

全局采⽤标记-整理算法

局部是copy

G1收集器的特短：
并⾏与并发
分代收集（与其他收集器⼀样）
空间整理（标记-整理算法）
可预测的停顿

如果不计算维护Remembered Set的操作，G1收集器的运作⼤致可划分为以下⼏个步骤：

- Initial Marking
- Concurrent Marking
- Final Marking
- Live Data Counting and Evacuation

Compact - Smaller, adjustable, but still a pause

Young GC all using copy and STW(smaller, adjustable).

CMS的写屏障实现是直接的同步操作，而G1就不得不将其实现为类似于消息队列的结构，把写前屏障和写后屏障中要做的事情都放到队列里，然后再异步处理

在发⽣Minor Gc之前，虚拟机会先检查⽼年代最⼤可⽤的连续空间是否⼤于新⽣代所有对象的总空间，如
果这个条件成⽴，那么Minnor Gc可⽤确保是安全的。如果不成⽴，则虚拟机会查看
HandlePromotyionFailure设置值是否允许担保失败。如果允许，那么会继续检查⽼年代最⼤空间是否⼤
于历次晋升到⽼年代对象的平均⼤⼩，如果⼤于，将尝试⼀次 Minnor Gc；如果⼩于，或者
HandlePromotyionFailure设置不允许冒险，那么这时要进⾏⼀次Full Gc

[JEP 345: NUMA-Aware Memory Allocation for G1](https://openjdk.java.net/jeps/345)

## marking

G1 marking uses an algorithm called Snapshot-At-The-Beginning (SATB) .
It takes a virtual snapshot of the heap at the time of the Initial Mark pause, when all objects that were live at the start of marking are considered live for the remainder of marking.
This means that objects that become dead (unreachable) during marking are still considered live for the purpose of space-reclamation (with some exceptions).
This may cause some additional memory wrongly retained compared to other collectors. However, SATB potentially provides better latency during the Remark pause.
The too conservatively considered live objects during that marking will be reclaimed during the next marking.

### Marking Data Structures

We maintain two marking bitmaps, labeled previous and next.
The previous marking bitmap is the last bitmap in which marking has been completed.
The next marking bitmap may be under construction.
The two physical bitmaps swap logical roles as marking is completed.
Each bitmap contains one bit for each address that can be the start of an object.
With the default 8-byte object alignment, this means 1 bitmap bit for every 64 heap bits.
We use a mark stack to hold (some of) the gray (marked but not yet recursively scanned) objects.

### Initial Marking Pause/Concurrent Marking

The first phase of a marking cycle clears the next marking bitmap. This is performed concurrently.
Next, the initial marking pause stops all mutator threads, and marks all objects directly reachable from the roots (in the generational mode, initial marking is in fact piggy-backed on a fullyyoung evacuation pause).
Each heap region contains two top at mark start (TAMS) variables, one for the previous marking and one for the next.
We will refer to these as the previous and next TAMS variables.
These variables are used to identify objects allocated during a marking phase.
These objects above a TAMS value are considered implicitly marked with respect to the marking to which the TAMS variable corresponds, but allocation is not slowed down by marking bitmap updates.
The initial marking pause iterates over all the regions in the heap, copying the current value of top in each region to the next TAMS of that region.
Steps A and D of figure 2 illustrate this.
Steps B and E of this figure show that objects allocated during concurrent marking are above the next TAMS value, and are thus considered live.
(The bitmaps physically cover the entire heap, but are shown only for the portions of regions for which they are relevant.)

Now mutator threads are restarted, and the concurrent phase of marking begins.
This phase is very similar to the concurrent marking phase: a “finger” pointer iterates over the marked bits.
Objects higher than the finger are implicitly gray; gray objects below the finger are represented with a mark stack.

The mutator may be updating the pointer graph as the collector is tracing it.
This mutation may remove a pointer in the “snapshot” object graph, violating the guarantee on which SATB marking is based.
Therefore, SATB marking requires mutator threads to record the values of pointer fields before they are overwritten.
Below we show pseudocode for the marking write barrier for a write of the value in rY to offset FieldOffset in an object whose address is in rX.
Its operation is explained below.

```
1| rTmp := load(rThread + MarkingInProgressOffset)
2| if (!rTmp) goto filtered
3| rTmp := load(rX + FieldOffset)
4| if (rTmp == null) goto filtered
5| call satb_enqueue(rTmp)
6| filtered:
```

The actual pointer store [rX, FieldOffset] := rY would follow.
The first two lines of the barrier skip the remainder if marking is not in progress; for many programs, this filters out a large majority of the dynamically executed barriers.
Lines 3 and 4 load the value in the object field, and check whether it is null. It is only necessary to log non-null values.
In many programs the majority of pointer writes are initializing writes to previously-null fields, so this further filtering is quite effective.
The satb enqueue operation adds the pointer value to the thread’s current marking buffer.
As with remembered set buffers, if the enqueue fills the buffer, it then adds it to the global set of completed marking buffers.
The concurrent marking thread checks the size of this set at regular intervals, interrupting its heap traversal to process filled buffers.

> [Use single mark bitmap in G1](https://bugs.openjdk.org/browse/JDK-8210708)

### Final Marking Pause

A marking phase is complete when concurrent marking has traversed all the marked objects and completely drained the mark stack, and when all logged updates have been processed.
The former condition is easy to detect; the latter is harder, since mutator threads “own” log buffers until they fill them.
The purpose of the stop-world final marking pause is to reach this termination condition reliably, while all mutator threads are stopped.
It is very simple: any unprocessed completed log buffers are processed as above, and the partially completed per-thread buffers are processed in the same way.
This process is done in parallel, to guard against programs with many mutator threads with partially filled marking log buffers causing long pause times or parallel scaling issues.

### Live Data Counting and Cleanup

Concurrent marking also counts the amount of marked data in each heap region.
Originally, this was done as part of the marking process.
However, evacuation pauses that move objects that are live must also update the per-region live data count.
When evacuation pauses are performed in parallel, and several threads are evacuating objects to the same region, updating this count consistently can be a source of parallel contention.
While a variety of techniques could have ameliorated this scaling problem, updating the count represented a significant portion of evacuation pause cost even with a single thread. Therefore, we opted to perform all live data counting concurrently.
When final marking is complete, the GC thread re-examines each region, counting the bytes of marked data below the TAMS value associated with the marking.
This is something like a sweeping phase, but note that we find live objects by examining the marking bitmap, rather than by traversing dead objects.

Evacuation pauses occurring during marking may increase the next TAMS value of some heap regions.
So a final stop-world cleanup pause is necessary to reliably finish this counting process.
This cleanup phase also completes marking in several other ways.
It is here that the next and previous bitmaps swap roles: the newly completed bitmap becomes the previous bitmap, and the old one is available for use in the next marking.
In addition, since the marking is complete, the value in the next TAMS field of each region is copied into the previous TAMS field, as shown in steps C and F of figure 2.
Liveness queries rely on the previous marking bitmap and the previous TAMS, so the newly-completed marking information will now be used to determine object liveness.
In figure 2, light gray indicates objects known to be dead.
Steps D and E show how the results of a completed marking may be used while a new marking is in progress.

Finally, the cleanup phase sorts the heap regions by expected GC efficiency.
This metric divides the marking’s estimate of garbage reclaimable by collecting a region by the cost of collecting it.
This cost is estimated based on a number of factors, including the estimated cost of evacuating the live data and the cost of traversing the region’s remembered set.
The result of this sorting is an initial ranking of regions by desirability for inclusion into collection sets.
The cost estimate can change over time, so this estimate is only initial.

Regions containing no live data whatsoever are immediately reclaimed in this phase.
For some programs, this method can reclaim a significant fraction of total garbage.

### Evacuation Pauses and Marking

First, an evacuation pause never evacuates an object that was proven dead in the last completed marking pass.
Since the object is dead, it obviously is not referenced from the roots, but it might be referenced from other dead objects.
References within the collection set are followed only if the referring object is found to be live.
References from outside the collection set are identified by the remembered sets; objects identified by the remembered sets are ignored if they have been shown to be dead.
Second, when we evacuate an object during an evacuation pause, we need to ensure that it is marked correctly, if necessary, with respect to both the previous and next markings.
It turns out that this is quite subtle and tricky.
Unfortunately, due to space restrictions, we cannot give here all the details of this interaction.

We allow evacuation pauses to occur when the marking thread’s marking stack is non-empty: if we did not, then marking could delay a desired evacuation pause by an arbitrary amount.
The marking stack entries may refer to objects in the collection set.
Since these objects are marked in the current marking, they are clearly live with respect to the previous marking, and may be evacuated by the evacuation pause.
To ensure that marking stack entries are updated properly, we treat the marking stack as a source of roots.

## Barrier

Generational garbage collectors need to keep track of references from older to younger generations so that younger generations can be garbage-collected without inspecting every object in the older generation(s).
The set of locations potentially containing pointers to newer objects is often called the *remembered set*.
At every store, the system must ensure that the updated location is added to the remembered set if the store creates a reference from an older to a newer object.
This mechanism is usually referred to as a *write barrier* or *store check*.

For some stores, the compiler can know statically that no store check is necessary, for example, when storing an integer (assuming that integers are implemented as immediates rather than as real heap-allocated objects).
However, in the general case, a store check must be executed for every store operation.
Since stores are fairly frequent in non-functional languages, an efficient write barrier implementation is essential.
The write barrier implementation described here reduces the write barrier overhead in the mutator to only two extra instructions per checked store.

### Card Marking

Our new write barrier implementation is based on Wilson’s card marking scheme.
In this scheme, the heap is divided into cards of size 2k words (typically, k = 5..7), and every card has an associated bit in a separate bit vector.
A store check simply marks the bit corresponding to the location being updated.
At garbage collection time, the collector scans the bit vector and, whenever it finds a marked bit, examines all pointers in the corresponding card in the heap.

Unfortunately, card marking as just described can be fairly slow.
Since the bit must be inserted into the bit vector, the corresponding word has to be read from memory, updated, and written back.
In addition, bit manipulations usually require several instructions on RISC processors.
Chambers and Ungar improved on Wilson’s scheme by using bytes instead of bits to mark cards.
That is, every card in the heap has one byte associated with it; a card is marked simply by storing a special value (e.g., zero) into the corresponding byte.
Although this scheme uses eight times more memory, the space overhead is usually still small; for example, with a card size of 32 words = 128 bytes, the space for the byte map is less than 1% of the heap size.

The big advantage of the byte marking scheme is its speed.
A store check requires just 3 SPARC instructions in addition to the actual store.

### Write Barrier

Our new write barrier improves on standard card marking by relaxing the invariant maintained by the card marking scheme.
The invariant maintained by standard card marking is

> bit or byte i is marked <--> card i may contain a pointer from old to new

Our scheme’s relaxed invariant is

> bit or byte i is marked <--> cards i..i+l may contain a pointer from old to new where l is a small constant (1 in our current implementation).

Essentially, l gives the store check some leeway in choosing which byte to mark—the marked byte may be up to l bytes away (in the direction of lower addresses) from the “correct” byte.
The relaxed invariant allows us to omit computing the exact address of the updated word: as long as the offset of the updated word (i.e., its distance from the beginning of the object) is less than $l*2^k$ ,
we can mark the byte corresponding to the object’s address rather than the byte corresponding to the updated field.
Thus, the common-case store check is only two instructions.

When scanning a card, the garbage collector simply needs to make sure that the last object in the card is scanned completely even if only part of it belongs to the card.
In other words, the collector continues scanning beyond the last word of the card until encountering the next object header.
However, with very large objects, this scheme could result in high scanning costs since the amount of scanning is no longer bounded by a fixed maximum leeway but only by the size of the last object.
Fortunately, this problem can be mitigated by marking the exact card for stores into arrays; then, if the last object starting in a card is an array, the collector need not continue scanning into the next card(s).

## G1

### G1CollectedHeap

```c

class G1CollectedHeap : public CollectedHeap {
  
  G1CollectorPolicy* _collector_policy;
  G1CardTable* _card_table;

  SoftRefPolicy      _soft_ref_policy;

  // These sets keep track of old, archive and humongous regions respectively.
  HeapRegionSet _old_set;
  HeapRegionSet _archive_set;
  HeapRegionSet _humongous_set;

  // The sequence of all heap regions in the heap.
  HeapRegionManager* _hrm;

  // Manages all allocations with regions except humongous object allocations.
  G1Allocator* _allocator;

  // The young region list.
  G1EdenRegions _eden;
  G1SurvivorRegions _survivor;

  // The current policy object for the collector.
  G1Policy* _g1_policy;
  G1HeapSizingPolicy* _heap_sizing_policy;

  G1CollectionSet _collection_set;
}
```

#### HeapRegionManager

```cpp

HeapRegionManager* HeapRegionManager::create_manager(G1CollectedHeap* heap, G1CollectorPolicy* policy) {
  if (policy->is_hetero_heap()) {
    return new HeterogeneousHeapRegionManager((uint)(policy->max_heap_byte_size() / HeapRegion::GrainBytes) /*heap size as num of regions*/);
  }
  return new HeapRegionManager();
}
```

G1HeapRegionTable

```cpp
class HeapRegionManager: public CHeapObj<mtGC> {
G1HeapRegionTable _regions;
G1RegionToSpaceMapper* _heap_mapper;
G1RegionToSpaceMapper* _prev_bitmap_mapper;
G1RegionToSpaceMapper* _next_bitmap_mapper;
FreeRegionList _free_list;
}
```

```cpp

class G1HeapRegionTable : public G1BiasedMappedArray<HeapRegion*> {
 protected:
  virtual HeapRegion* default_value() const { return NULL; }
};

```

#### HeapRegion

```cpp
class HeapRegion: public G1ContiguousSpace {
  private:
  // The remembered set for this region.
  // (Might want to make this "inline" later, to avoid some alloc failure
  // issues.)
  HeapRegionRemSet* _rem_set;

  protected:
  // The index of this region in the heap region sequence.
  uint  _hrm_index;

  HeapRegionType _type;

  // For a humongous region, region in which it starts.
  HeapRegion* _humongous_start_region;

  // True iff an attempt to evacuate an object in the region failed.
  bool _evacuation_failed;

  // Fields used by the HeapRegionSetBase class and subclasses.
  HeapRegion* _next;
  HeapRegion* _prev;
  
  // The index in the optional regions array, if this region
  // is considered optional during a mixed collections.
  uint _index_in_opt_cset;
  int  _young_index_in_cset;
  SurvRateGroup* _surv_rate_group;
  int  _age_index;
}
```

#### mem_allocate

```c
// share/gc/g1/g1CollectedHeap.cpp
HeapWord* G1CollectedHeap::mem_allocate(size_t word_size,
                              bool*  gc_overhead_limit_was_exceeded) {
  assert_heap_not_locked_and_not_at_safepoint();

  if (is_humongous(word_size)) {
    return attempt_allocation_humongous(word_size);
  }
  size_t dummy = 0;
  return attempt_allocation(word_size, word_size, &dummy);
}
```

```cpp
class G1Allocator : public CHeapObj<mtGC> {

}
```

## roots

```cpp

void G1RootProcessor::process_all_roots(OopClosure* oops,
                                        CLDClosure* clds,
                                        CodeBlobClosure* blobs) {
  AllRootsClosures closures(oops, clds);

  process_java_roots(&closures, NULL, 0);
  process_vm_roots(&closures, NULL, 0);

  process_code_cache_roots(blobs, NULL, 0);

  // refProcessor is not needed since we are inside a safe point
  _process_strong_tasks.all_tasks_claimed(G1RP_PS_refProcessor_oops_do);
}
```

### process_java_roots

```cpp

void G1RootProcessor::process_java_roots(G1RootClosures* closures,
                                         G1GCPhaseTimes* phase_times,
                                         uint worker_i) {
  // Iterating over the CLDG and the Threads are done early to allow us to
  // first process the strong CLDs and nmethods and then, after a barrier,
  // let the thread process the weak CLDs and nmethods.
  {
    G1GCParPhaseTimesTracker x(phase_times, G1GCPhaseTimes::CLDGRoots, worker_i);
    if (_process_strong_tasks.try_claim_task(G1RP_PS_ClassLoaderDataGraph_oops_do)) {
      ClassLoaderDataGraph::roots_cld_do(closures->strong_clds(), closures->weak_clds());
    }
  }

  {
    G1GCParPhaseTimesTracker x(phase_times, G1GCPhaseTimes::ThreadRoots, worker_i);
    bool is_par = n_workers() > 1;
```

`Threads::possibly_parallel_oops_do`

```
    Threads::possibly_parallel_oops_do(is_par,
                                       closures->strong_oops(),
                                       closures->strong_codeblobs());
  }
}
```

process_vm_roots

```cpp
void G1RootProcessor::process_vm_roots(G1RootClosures* closures,
                                       G1GCPhaseTimes* phase_times,
                                       uint worker_i) {
  OopClosure* strong_roots = closures->strong_oops();

  {
    G1GCParPhaseTimesTracker x(phase_times, G1GCPhaseTimes::UniverseRoots, worker_i);
    if (_process_strong_tasks.try_claim_task(G1RP_PS_Universe_oops_do)) {
      Universe::oops_do(strong_roots);
    }
  }

  {
    G1GCParPhaseTimesTracker x(phase_times, G1GCPhaseTimes::JNIRoots, worker_i);
    if (_process_strong_tasks.try_claim_task(G1RP_PS_JNIHandles_oops_do)) {
      JNIHandles::oops_do(strong_roots);
    }
  }

  {
    G1GCParPhaseTimesTracker x(phase_times, G1GCPhaseTimes::ObjectSynchronizerRoots, worker_i);
    if (_process_strong_tasks.try_claim_task(G1RP_PS_ObjectSynchronizer_oops_do)) {
      ObjectSynchronizer::oops_do(strong_roots);
    }
  }

  {
    G1GCParPhaseTimesTracker x(phase_times, G1GCPhaseTimes::ManagementRoots, worker_i);
    if (_process_strong_tasks.try_claim_task(G1RP_PS_Management_oops_do)) {
      Management::oops_do(strong_roots);
    }
  }

  {
    G1GCParPhaseTimesTracker x(phase_times, G1GCPhaseTimes::JVMTIRoots, worker_i);
    if (_process_strong_tasks.try_claim_task(G1RP_PS_jvmti_oops_do)) {
      JvmtiExport::oops_do(strong_roots);
    }
  }

#if INCLUDE_AOT
  if (UseAOT) {
    G1GCParPhaseTimesTracker x(phase_times, G1GCPhase
```

## collect

```cpp
// share/gc/g1/g1CollectedHeap.cpp
void G1CollectedHeap::collect(GCCause::Cause cause) {
  try_collect(cause, collection_counters(this));
}

bool G1CollectedHeap::try_collect(GCCause::Cause cause,
                                  const G1GCCounters& counters_before) {
  if (should_do_concurrent_full_gc(cause)) {
    return try_collect_concurrently(cause,
                                    counters_before.total_collections(),
                                    counters_before.old_marking_cycles_started());
  } else if (GCLocker::should_discard(cause, counters_before.total_collections())) {
    // Indicate failure to be consistent with VMOp failure due to
    // another collection slipping in after our gc_count but before
    // our request is processed.
    return false;
  } else if (cause == GCCause::_gc_locker || cause == GCCause::_wb_young_gc
             DEBUG_ONLY(|| cause == GCCause::_scavenge_alot)) {

    // Schedule a standard evacuation pause. We're setting word_size
    // to 0 which means that we are not requesting a post-GC allocation.
    VM_G1CollectForAllocation op(0,     /* word_size */
                                 counters_before.total_collections(),
                                 cause,
                                 policy()->max_pause_time_ms());
    VMThread::execute(&op);
    return op.gc_succeeded();
  } else {
    // Schedule a Full GC.
    VM_G1CollectFull op(counters_before.total_collections(),
                        counters_before.total_full_collections(),
                        cause);
    VMThread::execute(&op);
    return op.gc_succeeded();
  }
}
```

## service

```cpp

void G1ConcurrentMarkThread::run_service() {
  _vtime_start = os::elapsedVTime();

  while (wait_for_next_cycle()) {
    assert(in_progress(), "must be");

    GCIdMark gc_id_mark;
    FormatBuffer<128> title("Concurrent %s Cycle", _state == FullMark ? "Mark" : "Undo");
    GCTraceConcTime(Info, gc) tt(title);

    concurrent_cycle_start();

    if (_state == FullMark) {
      concurrent_mark_cycle_do();
    } else {
      assert(_state == UndoMark, "Must do undo mark but is %d", _state);
      concurrent_undo_cycle_do();
    }

    concurrent_cycle_end(_state == FullMark && !_cm->has_aborted());

    _vtime_accum = (os::elapsedVTime() - _vtime_start);
  }
  _cm->root_regions()->cancel_scan();
}
```

#### concurrent_mark_cycle

```cpp

void G1ConcurrentMarkThread::concurrent_mark_cycle_do() {
  HandleMark hm(Thread::current());
  ResourceMark rm;

  // Phase 1: Clear CLD claimed marks.
  phase_clear_cld_claimed_marks();

  // We have to ensure that we finish scanning the root regions
  // before the next GC takes place. To ensure this we have to
  // make sure that we do not join the STS until the root regions
  // have been scanned. If we did then it's possible that a
  // subsequent GC could block us from joining the STS and proceed
  // without the root regions have been scanned which would be a
  // correctness issue.
  //
  // So do not return before the scan root regions phase as a GC waits for a
  // notification from it.
  //
  // For the same reason ConcurrentGCBreakpoints (in the phase methods) before
  // here risk deadlock, because a young GC must wait for root region scanning.
  //
  // We can not easily abort before root region scan either because of the
  // reasons mentioned in G1CollectedHeap::abort_concurrent_cycle().

  // Phase 2: Scan root regions.
  if (phase_scan_root_regions()) return;

  // Phase 3: Actual mark loop.
  if (phase_mark_loop()) return;

  // Phase 4: Rebuild remembered sets.
  if (phase_rebuild_remembered_sets()) return;

  // Phase 5: Wait for Cleanup.
  if (phase_delay_to_keep_mmu_before_cleanup()) return;

  // Phase 6: Cleanup pause
```

### VMOperation

```cpp
void VM_G1TryInitiateConcMark::doit() {
  G1CollectedHeap* g1h = G1CollectedHeap::heap();

  GCCauseSetter x(g1h, _gc_cause);

  // Record for handling by caller.
  _terminating = g1h->concurrent_mark_is_terminating();

  if (_terminating && GCCause::is_user_requested_gc(_gc_cause)) {
    // When terminating, the request to initiate a concurrent cycle will be
    // ignored by do_collection_pause_at_safepoint; instead it will just do
    // a young-only or mixed GC (depending on phase).  For a user request
    // there's no point in even doing that much, so done.  For some non-user
    // requests the alternative GC might still be needed.
  } else if (!g1h->policy()->force_concurrent_start_if_outside_cycle(_gc_cause)) {
    // Failure to force the next GC pause to be a concurrent start indicates
    // there is already a concurrent marking cycle in progress.  Set flag
    // to notify the caller and return immediately.
    _cycle_already_in_progress = true;
  } else if ((_gc_cause != GCCause::_wb_breakpoint) &&
             ConcurrentGCBreakpoints::is_controlled()) {
    // WhiteBox wants to be in control of concurrent cycles, so don't try to
    // start one.  This check is after the force_concurrent_start_xxx so that a
    // request will be remembered for a later partial collection, even though
    // we've rejected this request.
    _whitebox_attached = true;
  } else if (!g1h->do_collection_pause_at_safepoint(_target_pause_time_ms)) {
    // Failure to perform the collection at all occurs because GCLocker is
    // active, and we have the bad luck to be the collection request that
    // makes a later _gc_locker collection needed.  (Else we would have hit
    // the GCLocker check in the prologue.)
    _transient_failure = true;
  } else if (g1h->should_upgrade_to_full_gc()) {
    _gc_succeeded = g1h->upgrade_to_full_collection();
  } else {
    _gc_succeeded = true;
  }
}
```

```cpp

void VM_G1CollectForAllocation::doit() {
  G1CollectedHeap* g1h = G1CollectedHeap::heap();

  if (should_try_allocation_before_gc() && _word_size > 0) {
    // An allocation has been requested. So, try to do that first.
    _result = g1h->attempt_allocation_at_safepoint(_word_size,
                                                   false /* expect_null_cur_alloc_region */);
    if (_result != NULL) {
      // If we can successfully allocate before we actually do the
      // pause then we will consider this pause successful.
      _gc_succeeded = true;
      return;
    }
  }

  GCCauseSetter x(g1h, _gc_cause);
  // Try a partial collection of some kind.
  _gc_succeeded = g1h->do_collection_pause_at_safepoint(_target_pause_time_ms);

  if (_gc_succeeded) {
    if (_word_size > 0) {
      // An allocation had been requested. Do it, eventually trying a stronger
      // kind of GC.
      _result = g1h->satisfy_failed_allocation(_word_size, &_gc_succeeded);
    } else if (g1h->should_upgrade_to_full_gc()) {
      // There has been a request to perform a GC to free some space. We have no
      // information on how much memory has been asked for. In case there are
      // absolutely no regions left to allocate into, do a full compaction.
      _gc_succeeded = g1h->upgrade_to_full_collection();
    }
  }
}
```

#### upgrade_to_full_collection

```cpp
bool G1CollectedHeap::upgrade_to_full_collection() {
  GCCauseSetter compaction(this, GCCause::_g1_compaction_pause);
  log_info(gc, ergo)("Attempting full compaction clearing soft references");
  bool success = do_full_collection(false /* explicit gc */,
                                    true  /* clear_all_soft_refs */,
                                    false /* do_maximum_compaction */);
  // do_full_collection only fails if blocked by GC locker and that can't
  // be the case here since we only call this when already completed one gc.
  assert(success, "invariant");
  return success;
}
```

```cpp

bool G1CollectedHeap::do_full_collection(bool explicit_gc,
                                         bool clear_all_soft_refs,
                                         bool do_maximum_compaction) {
  assert_at_safepoint_on_vm_thread();

  if (GCLocker::check_active_before_gc()) {
    // Full GC was not completed.
    return false;
  }

  const bool do_clear_all_soft_refs = clear_all_soft_refs ||
      soft_ref_policy()->should_clear_all_soft_refs();

  G1FullGCMark gc_mark;
  GCTraceTime(Info, gc) tm("Pause Full", NULL, gc_cause(), true);
  G1FullCollector collector(this, explicit_gc, do_clear_all_soft_refs, do_maximum_compaction);

  collector.prepare_collection();
  collector.collect();
  collector.complete_collection();

  // Full collection was successfully completed.
  return true;
}
```

## Young collect

```cpp

void G1YoungCollector::collect() {
  // Do timing/tracing/statistics/pre- and post-logging/verification work not
  // directly related to the collection. They should not be accounted for in
  // collection work timing.

  // The G1YoungGCTraceTime message depends on collector state, so must come after
  // determining collector state.
  G1YoungGCTraceTime tm(this, _gc_cause);

  // JFR
  G1YoungGCJFRTracerMark jtm(gc_timer_stw(), gc_tracer_stw(), _gc_cause);
  // JStat/MXBeans
  G1MonitoringScope ms(monitoring_support(),
                       false /* full_gc */,
                       collector_state()->in_mixed_phase() /* all_memory_pools_affected */);
  // Create the heap printer before internal pause timing to have
  // heap information printed as last part of detailed GC log.
  G1HeapPrinterMark hpm(_g1h);
  // Young GC internal pause timing
  G1YoungGCNotifyPauseMark npm(this);

  // Verification may use the workers, so they must be set up before.
  // Individual parallel phases may override this.
  set_young_collection_default_active_worker_threads();

  // Wait for root region scan here to make sure that it is done before any
  // use of the STW workers to maximize cpu use (i.e. all cores are available
  // just to do that).
  wait_for_root_region_scanning();

  G1YoungGCVerifierMark vm(this);
  {
    // Actual collection work starts and is executed (only) in this scope.

    // Young GC internal collection timing. The elapsed time recorded in the
    // policy for the collection deliberately elides verification (and some
    // other trivial setup above).
    policy()->record_young_collection_start();

    calculate_collection_set(jtm.evacuation_info(), _target_pause_time_ms);

    G1RedirtyCardsQueueSet rdcqs(G1BarrierSet::dirty_card_queue_set().allocator());
    G1PreservedMarksSet preserved_marks_set(workers()->active_workers());
    G1ParScanThreadStateSet per_thread_states(_g1h,
                                              &rdcqs,
                                              &preserved_marks_set,
                                              workers()->active_workers(),
                                              collection_set()->young_region_length(),
                                              collection_set()->optional_region_length(),
                                              &_evac_failure_regions);

    pre_evacuate_collection_set(jtm.evacuation_info(), &per_thread_states);

    bool may_do_optional_evacuation = collection_set()->optional_region_length() != 0;
    // Actually do the work...
    evacuate_initial_collection_set(&per_thread_states, may_do_optional_evacuation);

    if (may_do_optional_evacuation) {
      evacuate_optional_collection_set(&per_thread_states);
    }
    post_evacuate_collection_set(jtm.evacuation_info(), &per_thread_states);

    // Refine the type of a concurrent mark operation now that we did the
    // evacuation, eventually aborting it.
    _concurrent_operation_is_full_mark = policy()->concurrent_operation_is_full_mark("Revise IHOP");

    // Need to report the collection pause now since record_collection_pause_end()
    // modifies it to the next state.
    jtm.report_pause_type(collector_state()->young_gc_pause_type(_concurrent_operation_is_full_mark));

    policy()->record_young_collection_end(_concurrent_operation_is_full_mark, evacuation_failed());
  }
  TASKQUEUE_STATS_ONLY(print_taskqueue_stats());
  TASKQUEUE_STATS_ONLY(reset_taskqueue_stats());
}
```

### wait_for_root_region_scanning

```cpp


void G1YoungCollector::wait_for_root_region_scanning() {
  Ticks start = Ticks::now();
  // We have to wait until the CM threads finish scanning the
  // root regions as it's the only way to ensure that all the
  // objects on them have been correctly scanned before we start
  // moving them during the GC.
  bool waited = concurrent_mark()->root_regions()->wait_until_scan_finished();
  Tickspan wait_time;
  if (waited) {
    wait_time = (Ticks::now() - start);
  }
  phase_times()->record_root_region_scan_wait_time(wait_time.seconds() * MILLIUNITS);
}

```

### evacuate_initial_collection_set

```cpp

void G1YoungCollector::evacuate_initial_collection_set(G1ParScanThreadStateSet* per_thread_states,
                                                      bool has_optional_evacuation_work) {
  G1GCPhaseTimes* p = phase_times();

  {
    Ticks start = Ticks::now();
    rem_set()->merge_heap_roots(true /* initial_evacuation */);
    p->record_merge_heap_roots_time((Ticks::now() - start).seconds() * 1000.0);
  }

  Tickspan task_time;
  const uint num_workers = workers()->active_workers();

  Ticks start_processing = Ticks::now();
  {
    G1RootProcessor root_processor(_g1h, num_workers);
    G1EvacuateRegionsTask g1_par_task(_g1h,
                                      per_thread_states,
                                      task_queues(),
                                      &root_processor,
                                      num_workers,
                                      has_optional_evacuation_work);
    task_time = run_task_timed(&g1_par_task);
    // Closing the inner scope will execute the destructor for the
    // G1RootProcessor object. By subtracting the WorkerThreads task from the total
    // time of this scope, we get the "NMethod List Cleanup" time. This list is
    // constructed during "STW two-phase nmethod root processing", see more in
    // nmethod.hpp
  }
  Tickspan total_processing = Ticks::now() - start_processing;

  p->record_initial_evac_time(task_time.seconds() * 1000.0);
  p->record_or_add_nmethod_list_cleanup_time((total_processing - task_time).seconds() * 1000.0);

  rem_set()->complete_evac_phase(has_optional_evacuation_work);
}

```

## mixed collect

–XX:G1MixedGCLiveThresholdPercent
–XX:G1OldCSetRegionThresholdPercent

### G1ConcurrentMarkThread

create during `G1CollectedHeap::initialize`

```cpp
void ConcurrentGCThread::run() {
  // Wait for initialization to complete
  wait_init_completed();

  run_service();

  // Signal thread has terminated
  MonitorLocker ml(Terminator_lock);
  Atomic::release_store(&_has_terminated, true);
  ml.notify_all();
}
```

### need_to_start_conc_mark

```cpp
bool G1Policy::need_to_start_conc_mark(const char* source, size_t alloc_word_size) {
  if (about_to_start_mixed_phase()) {
    return false;
  }

  size_t marking_initiating_used_threshold = _ihop_control->get_conc_mark_start_threshold();

  size_t cur_used_bytes = _g1h->non_young_capacity_bytes();
  size_t alloc_byte_size = alloc_word_size * HeapWordSize;
  size_t marking_request_bytes = cur_used_bytes + alloc_byte_size;

  bool result = false;
  if (marking_request_bytes > marking_initiating_used_threshold) {
    result = collector_state()->in_young_only_phase() && !collector_state()->in_young_gc_before_mixed();
    log_debug(gc, ergo, ihop)("%s occupancy: " SIZE_FORMAT "B allocation request: " SIZE_FORMAT "B threshold: " SIZE_FORMAT "B (%1.2f) source: %s",
                              result ? "Request concurrent cycle initiation (occupancy higher than threshold)" : "Do not request concurrent cycle initiation (still doing mixed collections)",
                              cur_used_bytes, alloc_byte_size, marking_initiating_used_threshold, (double) marking_initiating_used_threshold / _g1h->capacity() * 100, source);
  }
  return result;
}
```

```cpp

void G1ConcurrentMarkThread::run_service() {
  _vtime_start = os::elapsedVTime();

  while (wait_for_next_cycle()) {
    assert(in_progress(), "must be");

    GCIdMark gc_id_mark;
    FormatBuffer<128> title("Concurrent %s Cycle", _state == FullMark ? "Mark" : "Undo");
    GCTraceConcTime(Info, gc) tt(title);

    concurrent_cycle_start();
```

concurrent_mark_cycle_do if FullMark

```
    if (_state == FullMark) {
      concurrent_mark_cycle_do();
    } else {
      assert(_state == UndoMark, "Must do undo mark but is %d", _state);
      concurrent_undo_cycle_do();
    }

    concurrent_cycle_end(_state == FullMark && !_cm->has_aborted());

    _vtime_accum = (os::elapsedVTime() - _vtime_start);
  }
  _cm->root_regions()->cancel_scan();
}
```

#### G1ParCopyClosure

```cpp
void G1ParCopyClosure<barrier, should_mark>::do_oop_work(T* p) {
  T heap_oop = RawAccess<>::oop_load(p);

  if (CompressedOops::is_null(heap_oop)) {
    return;
  }

  oop obj = CompressedOops::decode_not_null(heap_oop);

  assert(_worker_id == _par_scan_state->worker_id(), "sanity");

  const G1HeapRegionAttr state = _g1h->region_attr(obj);
  if (state.is_in_cset()) {
    oop forwardee;
    markWord m = obj->mark();
    if (m.is_marked()) {
      forwardee = cast_to_oop(m.decode_pointer());
    } else {
      forwardee = _par_scan_state->copy_to_survivor_space(state, obj, m);
    }
    assert(forwardee != NULL, "forwardee should not be NULL");
    RawAccess<IS_NOT_NULL>::oop_store(p, forwardee);

    if (barrier == G1BarrierCLD) {
      do_cld_barrier(forwardee);
    }
  } else {
    if (state.is_humongous()) {
      _g1h->set_humongous_is_live(obj);
    } else if ((barrier != G1BarrierNoOptRoots) && state.is_optional()) {
      _par_scan_state->remember_root_into_optional_region(p);
    }

    // The object is not in the collection set. should_mark is true iff the
    // current closure is applied on strong roots (and weak roots when class
    // unloading is disabled) in a concurrent mark start pause.
    if (should_mark) {
      mark_object(obj);
    }
  }
  trim_queue_partially();
}
```

#### G1CMConcurrentMarkingTask

```cpp
void work(uint worker_id) {
    ResourceMark rm;

    double start_vtime = os::elapsedVTime();

    {
      SuspendibleThreadSetJoiner sts_join;

      assert(worker_id < _cm->active_tasks(), "invariant");

      G1CMTask* task = _cm->task(worker_id);
      task->record_start_time();
      if (!_cm->has_aborted()) {
        do {
          task->do_marking_step(G1ConcMarkStepDurationMillis,
                                true  /* do_termination */,
                                false /* is_serial*/);

          _cm->do_yield_check();
        } while (!_cm->has_aborted() && task->has_aborted());
      }
      task->record_end_time();
      guarantee(!task->has_aborted() || _cm->has_aborted(), "invariant");
    }

    double end_vtime = os::elapsedVTime();
    _cm->update_accum_task_vtime(worker_id, end_vtime - start_vtime);
  }
```

### concurrent_mark_cycle_do

```cpp

void G1ConcurrentMarkThread::concurrent_mark_cycle_do() {
  HandleMark hm(Thread::current());
  ResourceMark rm;

  // Phase 1: Clear CLD claimed marks.
  phase_clear_cld_claimed_marks();

  // We have to ensure that we finish scanning the root regions
  // before the next GC takes place. To ensure this we have to
  // make sure that we do not join the STS until the root regions
  // have been scanned. If we did then it's possible that a
  // subsequent GC could block us from joining the STS and proceed
  // without the root regions have been scanned which would be a
  // correctness issue.
  //
  // So do not return before the scan root regions phase as a GC waits for a
  // notification from it.
  //
  // For the same reason ConcurrentGCBreakpoints (in the phase methods) before
  // here risk deadlock, because a young GC must wait for root region scanning.
  //
  // We can not easily abort before root region scan either because of the
  // reasons mentioned in G1CollectedHeap::abort_concurrent_cycle().

  // Phase 2: Scan root regions.
  if (phase_scan_root_regions()) return;

  // Phase 3: Actual mark loop.
  if (phase_mark_loop()) return;

  // Phase 4: Rebuild remembered sets.
  if (phase_rebuild_remembered_sets()) return;

  // Phase 5: Wait for Cleanup.
  if (phase_delay_to_keep_mmu_before_cleanup()) return;

  // Phase 6: Cleanup pause
  if (phase_cleanup()) return;

  // Phase 7: Clear bitmap for next mark.
  phase_clear_bitmap_for_next_mark();
}
```

### cleanup

```cpp
bool G1ConcurrentMarkThread::phase_cleanup() {
  CMCleanup cl(_cm);
  VM_G1Concurrent op(&cl, "Pause Cleanup");
  VMThread::execute(&op);
  return _cm->has_aborted();
}
```

```cpp

void G1ConcurrentMark::cleanup() {
  assert_at_safepoint_on_vm_thread();

  // If a full collection has happened, we shouldn't do this.
  if (has_aborted()) {
    return;
  }

  G1Policy* policy = _g1h->policy();
  policy->record_concurrent_mark_cleanup_start();

  double start = os::elapsedTime();

  verify_during_pause(G1HeapVerifier::G1VerifyCleanup, VerifyOption_G1UsePrevMarking, "Cleanup before");

  if (needs_remembered_set_rebuild()) {
    GCTraceTime(Debug, gc, phases) debug("Update Remembered Set Tracking After Rebuild", _gc_timer_cm);
    G1UpdateRemSetTrackingAfterRebuild cl(_g1h);
    _g1h->heap_region_iterate(&cl);
  } else {
    log_debug(gc, phases)("No Remembered Sets to update after rebuild");
  }

  verify_during_pause(G1HeapVerifier::G1VerifyCleanup, VerifyOption_G1UsePrevMarking, "Cleanup after");

  // We need to make this be a "collection" so any collection pause that
  // races with it goes around and waits for Cleanup to finish.
  _g1h->increment_total_collections();

  // Local statistics
  double recent_cleanup_time = (os::elapsedTime() - start);
  _total_cleanup_time += recent_cleanup_time;
  _cleanup_times.add(recent_cleanup_time);

  {
    GCTraceTime(Debug, gc, phases) debug("Finalize Concurrent Mark Cleanup", _gc_timer_cm);
    policy->record_concurrent_mark_cleanup_end(needs_remembered_set_rebuild());
  }
}
```

## Full collect

The current implementation of the full GC for G1 uses a single threaded mark-sweep-compact algorithm.
[JEP 307: Parallel Full GC for G1](https://openjdk.java.net/jeps/307)

```
-XX:ParallelGCThreads
```

```cpp

bool G1CollectedHeap::do_full_collection(bool explicit_gc,
                                         bool clear_all_soft_refs,
                                         bool do_maximum_compaction) {
  assert_at_safepoint_on_vm_thread();

  if (GCLocker::check_active_before_gc()) {
    // Full GC was not completed.
    return false;
  }

  const bool do_clear_all_soft_refs = clear_all_soft_refs ||
      soft_ref_policy()->should_clear_all_soft_refs();

  G1FullGCMark gc_mark;
  GCTraceTime(Info, gc) tm("Pause Full", NULL, gc_cause(), true);
  G1FullCollector collector(this, explicit_gc, do_clear_all_soft_refs, do_maximum_compaction);

  collector.prepare_collection();
  collector.collect();
  collector.complete_collection();

  // Full collection was successfully completed.
  return true;
}
```

### prepare_collection

```cpp

void G1FullCollector::prepare_collection() {
  _heap->policy()->record_full_collection_start();

  _heap->abort_concurrent_cycle();
  _heap->verify_before_full_collection(scope()->is_explicit_gc());

  _heap->gc_prologue(true);
  _heap->retire_tlabs();
  _heap->prepare_heap_for_full_collection();

  PrepareRegionsClosure cl(this);
  _heap->heap_region_iterate(&cl);

  reference_processor()->start_discovery(scope()->should_clear_soft_refs());

  // Clear and activate derived pointer collection.
  clear_and_activate_derived_pointers();
}
```

### G1FullCollector::collect

```cpp
// g1FullCollector.cpp
void G1FullCollector::collect() {
  phase1_mark_live_objects();
  verify_after_marking();

  // Don't add any more derived pointers during later phases
  deactivate_derived_pointers();

  phase2_prepare_compaction();

  phase3_adjust_pointers();

  phase4_do_compaction();
}
```

### phase1_mark_live_objects

```cpp

void G1FullCollector::phase1_mark_live_objects() {
  // Recursively traverse all live objects and mark them.
  GCTraceTime(Info, gc, phases) info("Phase 1: Mark live objects", scope()->timer());

```

Do the actual marking in [G1FullGCMarkTask](/docs/CS/Java/JDK/JVM/G1.md?id=G1FullGCMarkTask).

```
    G1FullGCMarkTask marking_task(this);
    run_task(&marking_task);
```

reference process

```
    uint old_active_mt_degree = reference_processor()->num_queues();
    reference_processor()->set_active_mt_degree(workers());
    GCTraceTime(Debug, gc, phases) debug("Phase 1: Reference Processing", scope()->timer());
    // Process reference objects found during marking.
    ReferenceProcessorPhaseTimes pt(scope()->timer(), reference_processor()->max_num_queues());
    G1FullGCRefProcProxyTask task(*this, reference_processor()->max_num_queues());
    const ReferenceProcessorStats& stats = reference_processor()->process_discovered_references(task, pt);
    scope()->tracer()->report_gc_reference_stats(stats);
    pt.print_all_references();
    assert(marker(0)->oop_stack()->is_empty(), "Should be no oops on the stack");

    reference_processor()->set_active_mt_degree(old_active_mt_degree);
  }
```

Weak oops cleanup.

```
    GCTraceTime(Debug, gc, phases) debug("Phase 1: Weak Processing", scope()->timer());
    WeakProcessor::weak_oops_do(_heap->workers(), &_is_alive, &do_nothing_cl, 1);
```

Class unloading and cleanup.

```
  if (ClassUnloading) {
    GCTraceTime(Debug, gc, phases) debug("Phase 1: Class Unloading and Cleanup", scope()->timer());
    // Unload classes and purge the SystemDictionary.
    bool purged_class = SystemDictionary::do_unloading(scope()->timer());
    _heap->complete_cleaning(&_is_alive, purged_class);
```

SymbolTable and StringTable

```
  scope()->tracer()->report_object_count_after_gc(&_is_alive);
}
```

#### G1FullGCMarkTask

```cpp
void G1FullGCMarkTask::work(uint worker_id) {
  Ticks start = Ticks::now();
  ResourceMark rm;
  G1FullGCMarker* marker = collector()->marker(worker_id);
  MarkingCodeBlobClosure code_closure(marker->mark_closure(), !CodeBlobToOopClosure::FixRelocations);

  if (ClassUnloading) {
    _root_processor.process_strong_roots(marker->mark_closure(),
                                         marker->cld_closure(),
                                         &code_closure);
  } else {
    _root_processor.process_all_roots(marker->mark_closure(),
                                      marker->cld_closure(),
                                      &code_closure);
  }

  // Mark stack is populated, now process and drain it.
  marker->complete_marking(collector()->oop_queue_set(), collector()->array_queue_set(), &_terminator);
  marker->flush_mark_stats_cache();

  // This is the point where the entire marking should have completed.
  assert(marker->oop_stack()->is_empty(), "Marking should have completed");
  assert(marker->objarray_stack()->is_empty(), "Array marking should have completed");
  log_task("Marking task", worker_id, start);
}
```

### phase2_prepare_compaction

```cpp
void G1FullCollector::phase2_prepare_compaction() {
  GCTraceTime(Info, gc, phases) info("Phase 2: Prepare for compaction", scope()->timer());
  G1FullGCPrepareTask task(this);
  run_task(&task);

  // To avoid OOM when there is memory left.
  if (!task.has_freed_regions()) {
    task.prepare_serial_compaction();
  }
}
```

#### G1FullGCPrepareTask

```cpp

void G1FullGCPrepareTask::work(uint worker_id) {
  Ticks start = Ticks::now();
  G1FullGCCompactionPoint* compaction_point = collector()->compaction_point(worker_id);
  G1CalculatePointersClosure closure(collector(), compaction_point);
  G1CollectedHeap::heap()->heap_region_par_iterate_from_start(&closure, &_hrclaimer);

  compaction_point->update();

  // Check if any regions was freed by this worker and store in task.
  if (closure.freed_regions()) {
    set_freed_regions();
  }
  log_task("Prepare compaction task", worker_id, start);
}
```

### phase3_adjust_pointers

```cpp
void G1FullCollector::phase3_adjust_pointers() {
  // Adjust the pointers to reflect the new locations
  GCTraceTime(Info, gc, phases) info("Phase 3: Adjust pointers", scope()->timer());

  G1FullGCAdjustTask task(this);
  run_task(&task);
}
```

#### G1FullGCAdjustTask

```cpp

void G1FullGCAdjustTask::work(uint worker_id) {
  Ticks start = Ticks::now();
  ResourceMark rm;

  // Adjust preserved marks first since they are not balanced.
  G1FullGCMarker* marker = collector()->marker(worker_id);
  marker->preserved_stack()->adjust_during_full_gc();

  {
    // Adjust the weak roots.
    AlwaysTrueClosure always_alive;
    _weak_proc_task.work(worker_id, &always_alive, &_adjust);
  }

  CLDToOopClosure adjust_cld(&_adjust, ClassLoaderData::_claim_strong);
  CodeBlobToOopClosure adjust_code(&_adjust, CodeBlobToOopClosure::FixRelocations);
  _root_processor.process_all_roots(&_adjust, &adjust_cld, &adjust_code);

  // Now adjust pointers region by region
  G1AdjustRegionClosure blk(collector(), worker_id);
  G1CollectedHeap::heap()->heap_region_par_iterate_from_worker_offset(&blk, &_hrclaimer, worker_id);
  log_task("Adjust task", worker_id, start);
}
```

### phase4_do_compaction

```cpp

void G1FullCollector::phase4_do_compaction() {
  // Compact the heap using the compaction queues created in phase 2.
  GCTraceTime(Info, gc, phases) info("Phase 4: Compact heap", scope()->timer());
  G1FullGCCompactTask task(this);
  run_task(&task);

  // Serial compact to avoid OOM when very few free regions.
  if (serial_compaction_point()->has_regions()) {
    task.serial_compaction();
  }
}
```

#### G1FullGCCompactTask

```cpp

void G1FullGCCompactTask::work(uint worker_id) {
  Ticks start = Ticks::now();
  GrowableArray<HeapRegion*>* compaction_queue = collector()->compaction_point(worker_id)->regions();
  for (GrowableArrayIterator<HeapRegion*> it = compaction_queue->begin();
       it != compaction_queue->end();
       ++it) {
    compact_region(*it);
  }

  G1ResetSkipCompactingClosure hc(collector());
  G1CollectedHeap::heap()->heap_region_par_iterate_from_worker_offset(&hc, &_claimer, worker_id);
  log_task("Compaction task", worker_id, start);
}
```

### complete_collection

```cpp

void G1FullCollector::complete_collection() {
  // Restore all marks.
  restore_marks();

  // When the pointers have been adjusted and moved, we can
  // update the derived pointer table.
  update_derived_pointers();

  _heap->concurrent_mark()->swap_mark_bitmaps();
  // Prepare the bitmap for the next (potentially concurrent) marking.
  _heap->concurrent_mark()->clear_next_bitmap(_heap->workers());

  _heap->prepare_heap_for_mutators();

  _heap->resize_all_tlabs();

  _heap->policy()->record_full_collection_end();
  _heap->gc_epilogue(true);

  _heap->verify_after_full_collection();
}
```

## Full GC

[JEP 307: Parallel Full GC for G1](https://openjdk.org/jeps/307)

```cpp
void G1FullCollector::collect() {
  phase1_mark_live_objects();
  verify_after_marking();

  // Don't add any more derived pointers during later phases
  deactivate_derived_pointers();

  phase2_prepare_compaction();

  phase3_adjust_pointers();

  phase4_do_compaction();
}
```

## Development

G1 is still under active development. Each JDK release sees several improvements to throughput, startup, pause times, and memory footprint.

* [Refactoring of remembered sets](https://bugs.openjdk.org/browse/JDK-8017163), which provided a significant memory footprint improvement
* [Increasing max region size](https://bugs.openjdk.org/browse/JDK-8275056)
* [Region pinning](https://openjdk.org/jeps/423)

### Comparison to Other Collectors

This is a summary of the main differences between G1 and the other collectors:

Parallel GC can compact and reclaim space in the old generation only as a whole. G1 incrementally distributes this work across multiple much shorter collections. This substantially shortens pause time at the potential expense of throughput.
G1 performs part of the old generation space-reclamation concurrently.
G1 may exhibit higher overhead than the above collectors, affecting throughput due to its concurrent nature.
ZGC is targeted at very large heaps, aiming to provide significantly smaller pause times at further cost of throughput.
Due to how it works, G1 has some unique mechanisms to improve garbage collection efficiency:

G1 can reclaim some completely empty, large areas of the old generation during any collection. This could avoid many otherwise unnecessary garbage collections, freeing a significant amount of space without much effort.
G1 can optionally try to deduplicate duplicate strings on the Java heap concurrently.
Reclaiming empty, large objects from the old generation is always enabled. You can disable this feature with the option -XX:-G1EagerReclaimHumongousObjects. String deduplication is disabled by default. You can enable it using the option -XX:+G1EnableStringDeduplication.

## Tuning

> 很多应用在升级JDK11，出现容器和Java进程内存整体变高的现象，主要源自Heap的使用率差异。
> CMS的Old generation为非移动式，由 CMSInitiatingOccupancyFraction 来控制使用比例来触发gc，因此应用启动后短时间内，heap old区使用率不会上升。
> 而G1的heap region是松散管理，整体利用heap，所以显得内存使用率高。本质是一个heap利用率的问题，cms初始留着部分heap不用。这个问题可以通过调低Xmx来解决

**The general recommendation is to use G1 with its default settings**, 
eventually giving it a different pause-time goal and setting a maximum Java heap size by using -Xmx if desired.

G1 defaults have been balanced differently than either of the other collectors.
G1's goals in the default configuration are neither maximum throughput nor lowest latency, but to provide relatively small, uniform pauses at high throughput.
However, G1's mechanisms to incrementally reclaim space in the heap and the pause-time control incur some overhead in both the application threads and in the space-reclamation efficiency.

If you prefer high throughput, then relax the pause-time goal by using -XX:MaxGCPauseMillis or provide a larger heap.
If latency is the main requirement, then modify the pause-time target.
Avoid limiting the young generation size to particular values by using options like -Xmn, -XX:NewRatio and others because the young generation size is the main means for G1 to allow it to meet the pause-time.
**Setting the young generation size to a single value overrides and practically disables pause-time control.**

Generally, when moving to G1 from other collectors, particularly the Concurrent Mark Sweep collector, start by removing all options that affect garbage collection, and only set the pause-time goal and overall heap size by using -Xmx and optionally -Xms.
Many options that are useful for other collectors to respond in some particular way, have either no effect at all, or even decrease throughput and the likelihood to meet the pause-time target.

For diagnosis purposes, G1 provides comprehensive logging.
A good start is to use the -Xlog:gc*=debug option and then refine the output from that if necessary.
The log provides a detailed overview during and outside the pauses about garbage collection activity.
This includes the type of collection and a breakdown of time spent in particular phases of the pause.

### Observing Full Garbage Collections

A full heap garbage collection (Full GC) is often very time consuming. Full GCs causedby too high heap occupancy in the old generation can be detected by finding the words Pause Full (G1 Compaction Pause) in the log. Full GCs are typically preceded by garbage collections that encounter an evacuation failure indicated by (Evacuation Failure) tags.

The reason that a Full GC occurs is because the application allocates too many objects that can't be reclaimed quickly enough. Often concurrent marking has not been able to complete in time to start a space-reclamation phase. The probability to run into a Full GC can be compounded by the allocation of many humongous objects. Due to the way these objects are allocated in G1, they may take up much more memory than expected.

The goal should be to ensure that concurrent marking completes on time. This can be achieved either by decreasing the allocation rate in the old generation, or giving the concurrent marking more time to complete.

G1 gives you several options to handle this situation better:

- You can determine the number of regions occupied by humongous objects on the Java heap using the gc+heap=info logging. 
  Y in the lines "Humongous regions: X->Y” give you the amount of regions occupied by humongous objects. 
  If this number is high compared to the number of old regions, the best option is to try to decrease this number of objects.  `To-space exhausted` Error please check if too many `Pause Young (Concurrent Start) (G1 Humongous Allocation)` in GC log.
  You can achieve this by increasing the region size using the  -XX:G1HeapRegionSize option.  8-32M
  The currently selected heap region size is printed at the beginning of the log.
- Increase the size of the Java heap. This typically increases the amount of time marking has to complete.
- Increase the number of concurrent marking threads by setting -XX:ConcGCThreads explicitly.
- Force G1 to start marking earlier. G1 automatically determines the Initiating Heap Occupancy Percent (IHOP) threshold based on earlier application behavior. 
  If the application behavior changes, these predictions might be wrong.
  There are two options: Lower the target occupancy for when to start space-reclamation by increasing the buffer used in an adaptive IHOP calculation by modifying -XX:G1ReservePercent; 
  or, disable the adaptive calculation of the IHOP by setting it manually using -XX:-G1UseAdaptiveIHOP and -XX:InitiatingHeapOccupancyPercent.

Other causes than Allocation Failure for a Full GC typically indicate that either the application or some external tool causes a full heap collection.
If the cause is System.gc(), and there is no way to modify the application sources, the effect of Full GCs can be mitigated by using -XX:+ExplicitGCInvokesConcurrent or let the VM completely ignore them by setting -XX:+DisableExplicitGC. External tools may still force Full GCs; they can be removed only by not requesting them.

### Humongous Object Fragmentation

A Full GC could occur before all Java heap memory has been exhausted due to the necessity of finding a contiguous set of regions for them. Potential options in this case are increasing the heap region size by using the option -XX:G1HeapRegionSize to decrease the number of humongous objects, or increasing size of the heap. In extreme cases, there might not be enough contiguous space available for G1 to allocate the object even if available memory indicates otherwise. This would lead to a VM exit if that Full GC can not reclaim enough contiguous space. As a result, there are no other options than either decreasing the amount of humongous object allocations as mentioned previously, or increasing the heap.

### Tuning for Latency

#### Unusual System or Real-Time Usage

For every garbage collection pause, the gc+cpu=info log output contains a line including information from the operating system with a breakdown about where during the pause-time has been spent.
An example for such output is User=0.19s Sys=0.00s Real=0.01s.

User time is time spent in VM code, system time is the time spent in the operating system, and real time is the amount of absolute time passed during the pause.
If the system time is relatively high, then most often the environment is the cause.

Common known issues for high system time are:

- The VM allocating or giving back memory from the operating system memory may cause unnecessary delays.
  Avoid the delays by setting minimum and maximum heap sizes to the same value using the options -Xms and -Xmx, and pre-touching all memory using -XX:+AlwaysPreTouch to move this work to the VM startup phase.
- Particularly in Linux, coalescing of small pages into huge pages by the Transparent Huge Pages (THP) feature tends to stall random processes, not just during a pause.
  Because the VM allocates and maintains a lot of memory, there is a higher than usual risk that the VM will be the process that stalls for a long time. Refer to the documentation of your operating system on how to disable the Transparent Huge Pages feature.
- Writing the log output may stall for some time because of some background task intermittently taking up all I/O bandwidth for the hard disk the log is written to.
  Consider using a separate disk for your logs or some other storage, for example memory-backed file system to avoid this.

Another situation to look out for is real time being a lot larger than the sum of the others this may indicate that the VM did not get enough CPU time on a possibly overloaded machine.

#### Reference Object Processing Takes Too Long

Information about the time taken for processing of Reference Objects is shown in the Reference Processing phase.
During the Reference Processing phase, G1 updates the referents of Reference Objects according to the requirements of the particular type of Reference Object.
By default, G1 tries to parallelize the sub-phases of Reference Processing using the following heuristic: for every -XX:ReferencesPerThread reference Objects start a single thread, bounded by the value in -XX:ParallelGCThreads.
This heuristic can be disabled by setting -XX:ReferencesPerThread to 0 to use all available threads by default, or parallelization disabled completely by -XX:-ParallelRefProcEnabled.

#### Young-Only Collections Within the Young-Only Phase Take Too Long

Normal young and, in general any young collection roughly takes time proportional to the size of the young generation, or more specifically, the number of live objects within the collection set that needs to be copied.
If the Evacuate Collection Set phase takes too long, in particular, the Object Copy sub-phase, decrease -XX:G1NewSizePercent. This decreases the minimum size of the young generation, allowing for potentially shorter pauses.

Another problem with sizing of the young generation may occur if application performance, and in particular the amount of objects surviving a collection, suddenly changes.
This may cause spikes in garbage collection pause time. It might be useful to decrease the maximum young generation size by using -XX:G1MaxNewSizePercent.
This limits the maximum size of the young generation and so the number of objects that need to be processed during the pause.

#### Mixed Collections Take Too Long

Mixed young collections are used to reclaim space in the old generation. The collection set of mixed collections contains young and old generation regions.
You can obtain information about how much time evacuation of either young or old generation regionscontribute to the pause-time by enabling the gc+ergo+cset=debug log output.


## Links

- [Garbage Collection](/docs/CS/Java/JDK/JVM/GC.md)

## References

1. [The Garbage-First Garbage Collector](http://www.oracle.com/technetwork/java/javase/tech/g1-intro-jsp-135488.html)
2. [Garbage-First Garbage Collector Tuning](http://www.oracle.com/technetwork/articles/java/g1gc-1984535.html)
3. [Garbage-First Garbage Collector Tuning - JDK11](https://docs.oracle.com/en/java/javase/11/gctuning/garbage-first-garbage-collector-tuning.html#GUID-90E30ACA-8040-432E-B3A0-1E0440AB556A)
4. [Garbage-First Garbage Collection](http://www.cs.williams.edu/~dbarowy/cs334s18/assets/p37-detlefs.pdf)
5. [A Fast Write Barrier for Generational Garbage Collectors](https://bibliography.selflanguage.org/_static/write-barrier.pdf)
6. [JEP 423: Region Pinning for G1](https://openjdk.org/jeps/423)
