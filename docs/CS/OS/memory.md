## Introduction

The address space of a process contains all of the memory state of the running program.


memory hierarchy

The part of the operating system that manages (part of) the memory hierarchy is called the memory manager.
Its job is to efficiently manage memory: keep track of which parts of memory are in use, allocate memory to processes when they need it, and deallocate it when they are done.

> [!NOTE]
> 
> Every address you see is virtual, only the OS (and the hardware) knows the real truth. 

### Goals

- One major goal of a virtual memory (VM) system is transparency. The OS should implement virtual memory in a way that is invisible to the running program.
- Another goal of VM is efficiency. The OS should strive to make the virtualization as efficient as possible, both in terms of time and space.
- Finally, a third VM goal is protection. The OS should make sure to protect processes from one another as well as the OS itself from processes.


exposing physical memory to processes has several major drawbacks.

- First, if user programs can address every byte of memory, they can easily trash the operating system, intentionally or by accident, bringing the system to a grinding halt.
  This problem exists even if only one user program (application) is running. Second, with this model, it is difficult to have multiple programs running at once.
- On personal computers, it is common to have sev eral programs open at once, one of them having the current focus, but the others being reactivated at the click of a mouse.
  Since this situation is difficult to achieve when there is no abstraction from physical memory, something had to be done.

Two problems have to be solved to allow multiple applications to be in memory at the same time without interfering with each other: protection and relocation.


### Dynamic (Hardware-based) Relocation


A better solution is to invent a new abstraction for memory: the address space.
Just as the process concept creates a kind of abstract CPU to run programs, the address space creates a kind of abstract memory for programs to live in.
An address space is the set of addresses that a process can use to address memory.
Each process has its own address space, independent of those belonging to other processes(except in some special circumstances where processes want to share their address spaces).


Specifically, we’ll need two hardware registers within each CPU: one is called the base register, and the other the bounds (sometimes called a limit register).
This base-and-bounds pair is going to allow us to place the address space anywhere we’d like in physical memory, and do so while ensuring that the process can only access its own address space.

In this setup, each program is written and compiled as if it is loaded at address zero.
However, when a program starts running, the OS decides where in physical memory it should be loaded and sets the base register to that value.
In the example above, the OS decides to load the process at physical address 32 KB and thus sets the base register to this value.

Interesting things start to happen when the process is running.
Now, when any memory reference is generated by the process, it is translated by the processor in the following manner:
```
physical address = virtual address + base
```

>
> The basic technique is referred to as static relocation, in which a piece of software known as the loader takes an executable that is about to be run and rewrites its addresses to the desired offset in physical memory.
>
> However, static relocation has numerous problems.
> - First and most importantly, it does not provide protection, as processes can generate bad addresses and thus illegally access other process’s or even OS memory
> - Another negative is that once placed, it is difficult to later relocate an address space to another location.


Each memory reference generated by the process is a **virtual address**; the hardware in turn adds the contents of the base register to this address and the result is a **physical address** that can be issued to the memory system.

Transforming a virtual address into a physical address is exactly the technique we refer to as **address translation**; that is, the hardware takes a virtual address the process thinks it is referencing and transforms it into a physical address which is where the data actually resides.
Because this relocation of the address happens at runtime, and because we can move address spaces even after the process has started running, the technique is often referred to as **dynamic relocation**.

A bounds (or limit) register ensures that such addresses are within the confines of the address space.


We should note that the base and bounds registers are hardware structures kept on the chip (one pair per CPU).
Sometimes people call the part of the processor that helps with address translation the **memory management unit** (**MMU**); as we develop more sophisticated memorymanagement techniques, we will be adding more circuitry to the MMU.


OS Requirements

- Memory management
  - Need to allocate memory for new processes;
  - Reclaim memory from terminated processes;
  - Generally manage memory via free list
- Base/bounds management Must set base/bounds properly upon context switch
- Exception handling Code to run when exceptions arise; likely action is to terminate offending process





## Segmentation


What if we tried to refer to an illegal address?

You can imagine what will happen: the hardware detects that the address is out of bounds, traps into the OS, likely leading to the termination of the offending process.
And now you know the origin of the famous term that all C programmers learn to dread: the segmentation violation or segmentation fault.

### Splitting and Coalescing


Segregated Lists

Buddy Allocation



## Paging

Instead of splitting up a process’s address space into some number of variable-sized logical segments (e.g., code, heap, stack), we divide it into *fixed-sized units*, each of which we call a page.


Most virtual memory systems use a technique called paging.
Addresses can be generated using indexing, base registers, segment registers, and other ways.

These program-generated addresses are called virtual addresses and form the virtual address space.
On computers without virtual memory, the virtual address is put directly onto the memory bus and causes the physical memory word with the same address to be read or written.
When virtual memory is used, the virtual addresses do not go directly to the memory bus.
Instead, they go to an MMU (Memory Management Unit) that maps the virtual addresses onto the physical memory addresses.

The virtual address space consists of fixed-size units called pages. The corresponding units in the physical memory are called page frames. The pages and page frames are generally the same size.

In the actual hardware, a *Present/absent bit* keeps track of which pages are physically present in memory.
What happens if the program references an unmapped address, the MMU notices that the page is unmapped (indicated by a cross in the figure) and causes the CPU to trap to the operating system.
This trap is called a *page fault*.
The operating system picks a little-used page frame and writes its contents back to the disk (if it is not already there).
It then fetches (also from the disk) the page that was just referenced into the page frame just freed, changes the map, and restarts the trapped instruction.

### Page Tables

To record where each virtual page of the address space is placed in physical memory, the operating system usually keeps a per-process data structure known as a **page table**.

In a simple implementation, the mapping of virtual addresses onto physical addresses can be summarized as follows: the virtual address is split into a virtual page number (high-order bits) and an offset (low-order bits).
For example, with a 16-bit address and a 4-KB page size, the upper 4 bits could specify one of the 16 virtual pages and the lower 12 bits would then specify the byte offset (0 to 4095) within the selected page.
However a split with 3 or 5 or some other number of bits for the page is also possible. Different splits imply different page sizes.

The virtual page number is used as an index into the page table to find the entry for that virtual page. From the page table entry, the page frame number (if any) is found.
The page frame number is attached to the high-order end of the offset, replacing the virtual page number, to form a physical address that can be sent to the memory.

Thus, the purpose of the page table is to map virtual pages onto page frames.
Mathematically speaking, the page table is a function, with the virtual page number as argument and the physical frame number as result.
Using the result of this function, the virtual page field in a virtual address can be replaced by a page frame field, thus forming a physical memory address.

#### Page Table Entry

he simplest form is called a linear page table, which is just an array.

The size varies from computer to computer, but 32 bits is a common size. The most important field is the Pa g e frame number.
After all, the goal of the page mapping is to output this value. Next to it we have the Present/absent bit. If this bit is 1, the entry is valid and can be used.
If it is 0, the virtual page to which the entry belongs is not currently in memory. Accessing a page table entry with this bit set to 0 causes a page fault.

The *Protection bits* tell what kinds of access are permitted. In the simplest form, this field contains 1 bit, with 0 for read/write and 1 for read only.
A more sophisticated arrangement is having 3 bits, one bit each for enabling reading, writing, and executing the page.

The *Modified* and *Referenced* bits keep track of page usage. When a page is written to, the hardware automatically sets the Modified bit.
This bit is of value when the operating system decides to reclaim a page frame. If the page in it has been modified (i.e., is ‘‘dirty’’), it must be written back to the disk.
If it has not been modified (i.e., is ‘‘clean’’), it can just be abandoned, since the disk copy is still valid.
The bit is sometimes called the *dirty bit*, since it reflects the page’s state.

The *Referenced bit* is set whenever a page is referenced, either for reading or for writing. Its value is used to help the operating system choose a page to evict when a page fault occurs.
Pages that are not being used are far better candidates than pages that are, and this bit plays an important role in several of the [page replacement algorithms](/docs/CS/OS/memory.md).

Finally, the last bit allows caching to be disabled for the page. This feature is important for pages that map onto device registers rather than memory.
If the operating system is sitting in a tight loop waiting for some I/O device to respond to a command it was just given, it is essential that the hardware keep fetching the word from the device, and not use an old cached copy.
With this bit, caching can be turned off. Machines that have a separate I/O space and do not use memory-mapped I/O do not need this bit.

Note that the disk address used to hold the page when it is not in memory is not part of the page table. The reason is simple.
The page table holds only that information the hardware needs to translate a virtual address to a physical address.
Information the operating system needs to handle page faults is kept in software tables inside the operating system. The hardware does not need it.

Virtual memory fundamentally does is create a new abstraction—the address space—which is an abstraction of physical memory, just as a process is an abstraction of the physical processor (CPU).
Virtual memory can be implemented by breaking the virtual address space up into pages, and mapping each one onto some page frame of physical memory or having it (temporarily) unmapped.

In any paging system, two major issues must be faced:

1. The mapping from virtual address to physical address must be fast.
2. If the virtual address space is large, the page table will be large.

The first point is a consequence of the fact that the virtual-to-physical mapping must be done on every memory reference.
All instructions must ultimately come from memory and many of them reference operands in memory as well.
Consequently, it is necessary to make one, two, or sometimes more page table references per instruction.

The second point follows from the fact that all modern computers use virtual addresses of at least 32 bits, with 64 bits becoming the norm for desktops and laptops.
With, say, a 4-KB page size, a 32-bit address space has 1 million pages, and a 64-bit address space has more than you want to contemplate.



##### Multilevel Page Tables

The secret to the multilevel page table method is to avoid keeping all the page tables in memory all the time.

##### Inverted Page Tables

An alternative to ever-increasing levels in a paging hierarchy is known as *inverted page tables*.
In this design, there is one entry per page frame in real memory, rather than one entry per page of virtual address space.

Although inverted page tables save lots of space, at least when the virtual address space is much larger than the physical memory, they have a serious downside: virtual-to-physical translation becomes much harder.
When process n references virtual page p, the hardware can no longer find the physical page by using p as an index into the page table. Instead, it must search the entire inverted page table for an entry (n, p).
Furthermore, this search must be done on every memory reference, not just on page faults. Searching a 256K table on every memory reference is not the way to make your machine blindingly fast.

The way out of this dilemma is to make use of the TLB. If the TLB can hold all of the heavily used pages, translation can happen just as fast as with regular page tables.
On a TLB miss, however, the inverted page table has to be searched in software.
One feasible way to accomplish this search is to have a hash table hashed on the virtual address. All the virtual pages currently in memory that have the same hash value are chained together.
If the hash table has as many slots as the machine has physical pages, the average chain will be only one entry long, greatly speeding up the mapping.
Once the page frame number has been found, the new (virtual, physical) pair is entered into the TLB.

### Paging Also Too Slow

Paging has many advantages over previous approaches (such as segmentation). 

- First, it does not lead to external fragmentation, as paging (by design) divides memory into fixed-sized units. 
- Second, it is quite flexible, enabling the sparse use of virtual address spaces.

However, implementing paging support without care will lead to a slower machine (with many extra memory accesses to access the page table) as well as memory waste (with memory filled with page tables instead of useful application data).





## TLB

To speed address translation, we are going to add what is called (for historical reasons) a translation-lookaside buffer, or TLB. 
A TLB is part of the chip’s memory-management unit (MMU), and is simply a hardware cache of popular virtual-to-physical address translations; thus, a better name would be an address-translation cache. 
Upon each virtual memory reference, the hardware first checks the TLB to see if the desired translation is held therein; if so, the translation is performed (quickly) without having to consult the page table (which has all translations).


Suppose the page walk does not find the page in the process’ page table and the program thus incurs a page fault. There are three possibilities.

- First, the page may actually be in memory, but not in this process’ page table. For instance, the page may have been brought in from disk by another process.
  In that case, we do not need to access the disk again, but merely map the page appropriately in the page tables.
  This is a pretty soft miss that is known as a *minor page fault*.
- Second, a *major page fault* occurs if the page needs to be brought in from disk.
- Third, it is possible that the program simply accessed an invalid address and no mapping needs to be added in the TLB at all.
  In that case, the operating system typically kills the program with a *segmentation fault*. Only in this case did the program do something wrong.
  All other cases are automatically fixed by the hardware and/or the operating system—at the cost of some performance.
  
> TLB valid bit != page table valid bit

In a page table, when a page-table entry (PTE) is marked invalid, it means that the page has not been allocated by the process, and should not be accessed by a correctly-working program. 
The usual response when an invalid page is accessed is to trap to the OS, which will respond by killing the process.

A TLB valid bit, in contrast, simply refers to whether a TLB entry has a valid translation within it. 
When a system boots, for example, a common initial state for each TLB entry is to be set to invalid, because no address translations are yet cached there. 
Once virtual memory is enabled, and once programs start running and accessing their virtual address spaces, the TLB is slowly populated, and thus valid entries soon fill the TLB.

The TLB valid bit is quite useful when performing a context switch too, as we’ll discuss further below. 
By setting all TLB entries to invalid, the system can ensure that the about-to-be-run process does not accidentally use a virtual-to-physical translation from a previous process.


### Context Switch


### Replacement Policy


One common approach is to evict the least-recently-used or LRU entry.

Another typical approach is to use a random policy, which evicts a TLB mapping at random.

> [!TIP]
> 
> RAM isn’t always RAM. 
> 
> Sometimes randomly accessing your address space, particularly if the number of pages accessed exceeds the TLB coverage, can lead to severe performance penalties.


However, TLBs do not make the world rosy for every program that exists.

In particular, if the number of pages a program accesses in a short period of time exceeds the number of pages that fit into the TLB, the program will generate a large number of TLB misses, and thus run quite a bit more slowly. 
We refer to this phenomenon as exceeding the TLB coverage, and it can be quite a problem for certain programs. 
One solution is to include support for larger page sizes; by mapping key data structures into regions of the program’s address space that are mapped by larger pages, the effective coverage of the TLB can be increased. 
Support for large pages is often exploited by programs such as a database management system (a DBMS), which have certain data structures that are both large and randomly-accessed.


One other TLB issue worth mentioning: TLB access can easily become a bottleneck in the CPU pipeline, in particular with what is called a physically-indexed cache.
With such a cache, address translation has to take place before the cache is accessed, which can slow things down quite a bit. 
Because of this potential problem, people have looked into all sorts of clever ways to access caches with virtual addresses, thus avoiding the expensive step of translation in the case of a cache hit. 
Such a virtually indexed cache solves some performance problems, but introduces new issues into hardware design as well.



## Managing Free Memory

When memory is assigned dynamically, the operating system must manage it.
In general terms, there are two ways to keep track of memory usage: bitmaps and free lists.




## Swapping

Tw o general approaches to dealing with memory overload have been developed over the years.
The simplest strategy, called swapping, consists of bringing in each process in its entirety, running it for a while, then putting it back on the disk.
Idle processes are mostly stored on disk, so they do not take up any memory when they are not running (although some of them wake up periodically to do their work, then go to sleep again).
The other strategy, called virtual memory, allows programs to run even when they are only partially in main memory.

When swapping creates multiple holes in memory, it is possible to combine them all into one big one by moving all the processes downward as far as possible.
This technique is known as memory compaction. It is usually not done because it requires a lot of CPU time.

If it is expected that most processes will grow as they run, it is probably a good idea to allocate a little extra memory whenever a process is swapped in or moved,
to reduce the overhead associated with moving or swapping processes that no longer fit in their allocated memory.
However, when swapping processes to disk, only the memory actually in use should be swapped; it is wasteful to swap the extra memory as well.

If processes can have two growing segments—for example, the data segment being used as a heap for variables that are dynamically allocated and released and a stack segment for the normal local variables and return addresses—an alternative arrangement suggests itself.
In this figure we see that each process illustrated has a stack at the top of its allocated memory that is growing downward, and a data segment just beyond the program text that is growing upward.
The memory between them can be used for either segment.
If it runs out, the process will either have to be moved to a hole with sufficient space, swapped out of memory until a large enough hole can be created, or killed.


### Swap Space


The first thing we will need to do is to reserve some space on the disk for moving pages back and forth. 
In operating systems, we generally refer to such space as swap space, because we swap pages out of memory to it and swap pages into memory from it.

The size of the swap space is important, as ultimately it determines the maximum number of memory pages that can be in use by a system at a given time.





## Page Replacement Algorithms

When a page fault occurs, the operating system has to choose a page to evict(remove from memory) to make room for the incoming page.
If the page to be removed has been modified while in memory, it must be rewritten to the disk to bring the disk copy up to date.
If, however, the page has not been changed (e.g., it contains program text), the disk copy is already up to date, so no rewrite is needed.
The page to be read in just overwrites the page being evicted.

While it would be possible to pick a random page to evict at each page fault, system performance is much better if a page that is not heavily used is chosen.
If a heavily used page is removed, it will probably have to be brought back in quickly, resulting in extra overhead.

### Optimal Algorithm

The best possible page replacement algorithm is easy to describe but impossible to actually implement.

The only problem with this algorithm is that it is unrealizable. At the time of the page fault, the operating system has no way of knowing when each of the pages will be referenced next.
(We saw a similar situation earlier with the [shortest-job-first scheduling algorithm](/docs/CS/OS/process.md?id=shortest-job-first)—how can the system tell which job is shortest?)
Still, by running a program on a simulator and keeping track of all page references, it is possible to implement optimal page replacement on the second run by using the page-reference information collected during the first run.

### NRU Algorithm

The NRU (Not Recently Used) algorithm removes a page at random from the lowest-numbered nonempty class.
Implicit in this algorithm is the idea that it is better to remove a modified page that has not been referenced in at least one clock tick (typically about 20 msec) than a clean page that is in heavy use.
The main attraction of NRU is that it is easy to understand, moderately efficient to implement, and gives a performance that, while certainly not optimal, may be adequate.

### FIFO Algorithm

### Second Chance Algorithm

A simple modification to FIFO that avoids the problem of throwing out a heavily used page is to inspect the R bit of the oldest page.
If it is 0, the page is both old and unused, so it is replaced immediately.
If the R bit is 1, the bit is cleared, the page is put onto the end of the list of pages, and its load time is updated as though it had just arrived in memory. Then the search continues.
The operation of this algorithm, called second chance.

### Clock Algorithm

Although second chance is a reasonable algorithm, it is unnecessarily inefficient because it is constantly moving pages around on its list.
A better approach is to keep all the page frames on a circular list in the form of a clock. The hand points to the oldest page.

### LRU Algorithm

A good approximation to the optimal algorithm is based on the observation that pages that have been heavily used in the last few instructions will probably be heavily used again soon.
Conversely, pages that have not been used for ages will probably remain unused for a long time.
This idea suggests a realizable algorithm: when a page fault occurs, throw out the page that has been unused for the longest time.
This strategy is called *LRU (Least Recently Used)* paging.

Although LRU is theoretically realizable, it is not cheap by a long shot.
To fully implement LRU, it is necessary to maintain a linked list of all pages in memory, with the most recently used page at the front and the least recently used page at the rear.
The difficulty is that the list must be updated on every memory reference.
Finding a page in the list, deleting it, and then moving it to the front is a very time consuming operation, even in hardware (assuming that such hardware could be built).

### Working Set Algorithm

In the purest form of paging, processes are started up with none of their pages in memory.
As soon as the CPU tries to fetch the first instruction, it gets a page fault, causing the operating system to bring in the page containing the first instruction.
Other page faults for global variables and the stack usually follow quickly.
After a while, the process has most of the pages it needs and settles down to run with relatively few page faults.
This strategy is called *demand paging* because pages are loaded only on demand, not in advance.

The set of pages that a process is currently using is its working set.
If the entire working set is in memory, the process will run without causing many faults until it moves into another execution phase (e.g., the next pass of the compiler).
If the available memory is too small to hold the entire working set, the process will cause many page faults and run slowly,
since executing an instruction takes a few nanoseconds and reading in a page from the disk typically takes 10 msec At a rate of one or two instructions per 10 msec, it will take ages to finish.
A program causing page faults every few instructions is said to be *thrashing*.

Therefore, many paging systems try to keep track of each process’ working set and make sure that it is in memory before letting the process run. This approach is called the *working set model*.
It is designed to greatly reduce the page fault rate. Loading the pages before letting processes run is also called *prepaging*. Note that the working set changes over time.

### WSClock Algorithm

### Summary of Page Replacement Algorithms


| Algorithm                                                  | Comment                                        |
| ------------------------------------------------------------ | ------------------------------------------------ |
| Optimal                                                    | Not implementable, but useful as a benchmark   |
| NRU (Not Recently Used)                                    | Very crude approximation of LRU                |
| FIFO (First-In, First-Out) Might throw out important pages |                                                |
| Second chance                                              | Big improvement over FIFO                      |
| Clock                                                      | Realistic                                      |
| LRU (Least Recently Used)                                  | Excellent, but difficult to implement exactly  |
| NFU (Not Frequently Used)                                  | Fairly crude approximation to LRU              |
| Aging                                                      | Efficient algorithm that approximates LRU well |
| Working set                                                | Somewhat expensive to implement                |
| WSClock                                                    | Good efficient algorithm                       |

All in all, the two best algorithms are aging and WSClock. They are based on LRU and the working set, respectively. 
Both give good paging performance and can be implemented efficiently.





## Links

- [Operating Systems](/docs/CS/OS/OS.md)
